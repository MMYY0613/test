import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from pathlib import Path
import itertools
import warnings
import seaborn as sns
from scipy.optimize import lsq_linear
import matplotlib.dates as mdates
from matplotlib import patheffects as pe

warnings.simplefilter('ignore')

# =========================================================
# 1. è¨­å®šãƒ»å®šæ•°
# =========================================================
CONFIG = {
    "data_path": "./data/all_q_merged_new_tmp.csv",
    "output_dir": "./output_mw_new_tmp_3",
    "test_steps": 4,
    "pc_max": 3,
    "p_lag": 1,
    "ridge": 1.0,
    "do_irf": True,
    "verbose": False,
}

BASE_LEVELS = {
    "GDP": 500000, "NIKKEI": 20000, "USD_JPY": 150, "UNEMP_RATE": 3.0,
    "JGB_1Y": 0.0, "JGB_3Y": 0.0, "JGB_10Y": 0.0, "CPI": 0.0, "TOPIX": 1500,
}

TARGET_MACRO = list(BASE_LEVELS.keys())

SECTOR_COLS = [
    "ã‚¬ãƒ©ã‚¹ãƒ»åœŸçŸ³è£½å“",
    "ã‚´ãƒ è£½å“",
    "é›»æ°—æ©Ÿå™¨",
    "é‡‘å±è£½å“",
    "ãã®ä»–è£½å“",
    "æ©Ÿæ¢°",
    "é£Ÿæ–™å“",
    "è¼¸é€ç”¨æ©Ÿå™¨",
    "åŒ–å­¦",
    "é›»æ°—ãƒ»ã‚¬ã‚¹æ¥­",
    "é‰±æ¥­",
    "é‰„é‹¼",
    "ç²¾å¯†æ©Ÿå™¨",
    "çŸ³æ²¹ãƒ»çŸ³ç‚­è£½å“",
]

mpl.rcParams["axes.unicode_minus"] = False
plt.rcParams['font.family'] = ["Hiragino Sans"]

# =========================================================
# 2. ãƒ­ã‚¸ãƒƒã‚¯é–¢æ•°
# =========================================================
def log(*args, **kwargs):
    if CONFIG.get("verbose", True):
        print(*args, **kwargs)

def smart_transform(series, name):
    # CPIãŒå‰å¹´æ¯”(%)ãªã‚‰ â€œãã®ã¾ã¾â€ ä½¿ã†ï¼ˆå¤‰æ›ã—ãªã„ï¼‰
    if name == "CPI":
        return series, "LEVEL"

    if (series <= 0).any() or "JGB" in name or "UNEMP" in name:
        return series.diff(), "DIFF"
    return np.log(series).diff(), "LOGDIFF"

def prepare_aligned(df_raw):
    m_work = pd.DataFrame(index=df_raw.index)
    meta = {}
    for col in TARGET_MACRO:
        if col in df_raw.columns:
            ts, method = smart_transform(df_raw[col], col)
            m_work[f"{col}_{method}"] = ts
            meta[f"{col}_{method}"] = {"orig": col, "method": method}
    m_df = m_work.dropna()
    s_raw = df_raw[[c for c in SECTOR_COLS if c in df_raw.columns]].interpolate(limit_direction='both')
    s_diff = s_raw.diff().dropna()
    common = s_diff.index.intersection(m_df.index)
    return s_diff.loc[common], m_df.loc[common], common, meta

def make_design(endog, exog, p):
    Y = endog.values
    X_parts = [np.ones((len(endog), 1))]
    for lag in range(1, p+1):
        X_parts.append(endog.shift(lag).values)
    if exog is not None and not exog.empty:
        X_parts.append(exog.values)
    X = np.concatenate(X_parts, axis=1)
    valid = ~np.isnan(X).any(axis=1) & ~np.isnan(Y).any(axis=1)
    return Y[valid], X[valid]

def analyze_pca_details(pca, scaler, sector_df, pc_cols, root_dir, tr_idx_list, te_idx_list):
    pca_dir = root_dir / "pca_analysis"
    pca_dir.mkdir(exist_ok=True)

    idx = np.r_[tr_idx_list, te_idx_list]
    X_target = sector_df.iloc[idx].sort_index()
    X_scaled = scaler.transform(X_target)
    scores = pca.transform(X_scaled)
    score_df = pd.DataFrame(scores, index=X_target.index, columns=pc_cols)

    expl = pca.explained_variance_ratio_
    components = pca.components_.copy()

    score_df.to_csv(pca_dir / "ä¸»æˆåˆ†ã‚¹ã‚³ã‚¢_ç”Ÿãƒ‡ãƒ¼ã‚¿.csv", encoding="utf-8-sig")

    loadings = pd.DataFrame(components.T, index=sector_df.columns, columns=pc_cols)
    loadings.to_csv(pca_dir / "ã‚»ã‚¯ã‚¿ãƒ¼ã®è² è·é‡_ä¸€è¦§.csv", encoding="utf-8-sig")

    expl_df = pd.DataFrame(expl.reshape(1, -1), columns=pc_cols, index=["ExplainedVariance"])
    expl_df.to_csv(pca_dir / "ä¸»æˆåˆ†_å¯„ä¸ç‡.csv", encoding="utf-8-sig")

    # ã‚¹ã‚³ã‚¢æ¨ç§»ãƒ—ãƒ­ãƒƒãƒˆï¼ˆPC3ã¾ã§ï¼‰
    plt.figure(figsize=(12, 5))
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']
    for i, pc in enumerate(pc_cols[:3]):
        plt.plot(score_df.index, score_df[pc], label=f"{pc} (å¯„ä¸:{expl[i]*100:.1f}%)", color=colors[i], lw=2)
    plt.title("ä¸»æˆåˆ†ã‚¹ã‚³ã‚¢ã®æ¨ç§» (ã‚»ã‚¯ã‚¿ãƒ¼æ¨™æº–åŒ–å¾Œã®ç”Ÿå€¤)")

    # --- è»¸ãƒ©ãƒ™ãƒ«è¿½åŠ  ---
    plt.xlabel("æ—¥ä»˜")
    plt.ylabel("ä¸»æˆåˆ†ã‚¹ã‚³ã‚¢")

    plt.axhline(0, color='black', lw=1); plt.legend(loc='upper left', bbox_to_anchor=(1, 1)); plt.grid(axis='y', alpha=0.3); plt.tight_layout()
    plt.savefig(pca_dir / "PCA_ã‚¹ã‚³ã‚¢æ¨ç§»_ç”Ÿå€¤.png"); plt.close()

    # è² è·é‡ã®æ£’ã‚°ãƒ©ãƒ•ï¼ˆPC3ã¾ã§ï¼‰
    for i, pc in enumerate(pc_cols[:3]):
        plt.figure(figsize=(10, 5))
        loadings[pc].reindex(loadings[pc].abs().sort_values(ascending=False).index).head(10).plot(kind='bar', color=colors[i])
        plt.title(f"{pc} ã‚»ã‚¯ã‚¿ãƒ¼ã®è² è·é‡ (å¯„ä¸:{expl[i]*100:.1f}%)")

        # --- è»¸ãƒ©ãƒ™ãƒ«è¿½åŠ  ---
        plt.xlabel("ã‚»ã‚¯ã‚¿ãƒ¼")
        plt.ylabel("è² è·é‡")

        plt.axhline(0, color='black', lw=1); plt.xticks(rotation=45, ha='right'); plt.grid(axis='y', alpha=0.3); plt.tight_layout()
        plt.savefig(pca_dir / f"è² è·é‡_{pc}_TOP10.png"); plt.close()

    print(f"âœ… PCAåˆ†æå®Œäº†: {pca_dir}")

def plot_abs_heatmaps(full_pca_df, output_root):
    # PCã”ã¨ã«ãƒ«ãƒ¼ãƒ—ã—ã¦å¯è¦–åŒ–
    for pc in ["PC1", "PC2", "PC3"]:
        target_df = full_pca_df[full_pca_df["PC_Type"] == pc]
        if target_df.empty: continue

        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã—ã¦ã€ã‚»ã‚¯ã‚¿ãƒ¼åˆ—ã®ã¿æŠ½å‡ºï¼ˆæ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
        # æ–‡å­—åˆ—ã‚«ãƒ©ãƒ ã‚’é™¤å¤–ã—ã¦è»¢ç½®
        plot_data = target_df.drop(columns=["PC_Type", "Window", "Explained_Variance"]).T
        plot_data = plot_data.abs()
        plot_data.columns = target_df["Window"] # xè»¸ã‚’ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åã«

        plt.figure(figsize=(14, 9))

        # è² è·é‡ã®çµ¶å¯¾å€¤ãªã®ã§ã€0ã‹ã‚‰ã®å¼·å¼±ãŒã¯ã£ãã‚Šã™ã‚‹ "Reds" ãªã©ã‚’æ¡ç”¨
        # vmin=0, vmax=0.5 ãªã©ã§ã‚¹ã‚±ãƒ¼ãƒ«ã‚’å›ºå®šã™ã‚‹ã¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é–“ã®æ¯”è¼ƒãŒã—ã‚„ã™ããªã‚Šã¾ã™
        sns.heatmap(plot_data,
                    cmap="Reds",
                    annot=False,
                    cbar_kws={'label': f'{pc} è² è·é‡ï¼ˆçµ¶å¯¾å€¤ï¼‰'},
                    vmin=0,
                    vmax=max(0.5, plot_data.max().max()))

        # å¯„ä¸ç‡ã‚’å–å¾—
        mean_var = target_df["Explained_Variance"].mean() * 100
        plt.title(f"{pc} æ§‹æˆã‚»ã‚¯ã‚¿ãƒ¼ã®è² è·é‡æ¨ç§»ï¼ˆçµ¶å¯¾å€¤ï¼‰\n[å¹³å‡å¯„ä¸ç‡: {mean_var:.1f}%]", fontsize=14)
        plt.xlabel("ãƒ†ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ (Window)", fontsize=12)
        plt.ylabel("ã‚»ã‚¯ã‚¿ãƒ¼", fontsize=12)

        plt.tight_layout()
        plt.savefig(output_root / f"å…¨æœŸé–“_{pc}_æ§‹é€ æ¨ç§»_çµ¶å¯¾å€¤.png")
        plt.close()

def plot_signed_heatmaps(full_pca_df, output_root):
    for pc in ["PC1", "PC2", "PC3"]:
        target_df = full_pca_df[full_pca_df["PC_Type"] == pc]
        if target_df.empty:
            continue

        plot_data = target_df.drop(columns=["PC_Type", "Window", "Explained_Variance"]).T
        plot_data.columns = target_df["Window"]

        vmax = np.nanmax(np.abs(plot_data.values))
        vmax = max(0.5, float(vmax))  # è¦‹ã‚„ã™ã•ã®ä¸‹é™

        plt.figure(figsize=(14, 9))
        sns.heatmap(
            plot_data,
            cmap="RdBu_r",
            center=0,
            vmin=-vmax,
            vmax=vmax,
            annot=False,
            cbar_kws={"label": f"{pc} è² è·é‡ï¼ˆç¬¦å·ã¤ãï¼‰"},
        )

        mean_var = target_df["Explained_Variance"].mean() * 100
        plt.title(f"{pc} æ§‹æˆã‚»ã‚¯ã‚¿ãƒ¼ã®è² è·é‡æ¨ç§»ï¼ˆç¬¦å·ã¤ãï¼‰\n[å¹³å‡å¯„ä¸ç‡: {mean_var:.1f}%]", fontsize=14)
        plt.xlabel("ãƒ†ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ (Window)")
        plt.ylabel("ã‚»ã‚¯ã‚¿ãƒ¼")
        plt.tight_layout()
        plt.savefig(output_root / f"å…¨æœŸé–“_{pc}_æ§‹é€ æ¨ç§»_ç¬¦å·ã¤ã.png")
        plt.close()

def aggregate_results(output_root, agg_root=None):
    output_root = Path(output_root)
    agg_root = Path(agg_root) if agg_root is not None else output_root
    all_summaries = []
    pca_abs_details = []

    # 1. å„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’å·¡å›ã—ã¦ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’åé›†
    for window_path in sorted(output_root.glob("Window_Test_*")):
        window_name = window_path.name.replace("Window_Test_", "")

        # ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚µãƒãƒªãƒ¼
        summary_file = window_path / "ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚µãƒãƒªãƒ¼.csv"
        if summary_file.exists():
            df = pd.read_csv(summary_file)
            df["Test_Window"] = window_name
            all_summaries.append(df)

        # PCAè² è·é‡ï¼ˆçµ¶å¯¾å€¤ï¼‰
        loadings_file = window_path / "pca_analysis" / "ã‚»ã‚¯ã‚¿ãƒ¼ã®è² è·é‡_ä¸€è¦§.csv"
        variance_file = window_path / "pca_analysis" / "ä¸»æˆåˆ†_å¯„ä¸ç‡.csv"
        if loadings_file.exists() and variance_file.exists():
            ld_df = pd.read_csv(loadings_file, index_col=0)
            vr_df = pd.read_csv(variance_file, index_col=0)
            for pc in ["PC1", "PC2", "PC3"]:
                if pc in ld_df.columns:
                    row = ld_df[pc].to_frame().T   # â† abs() ã‚’æ¶ˆã™
                    row.index = [f"{window_name}_{pc}"]
                    row.insert(0, "Window", window_name)
                    row.insert(1, "PC_Type", pc)
                    row.insert(2, "Explained_Variance", vr_df.at["ExplainedVariance", pc])
                    pca_abs_details.append(row)

    if not all_summaries:
        return

    # --- 2. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã®è©³ç´°çµ±è¨ˆï¼ˆå¿˜ã‚Œã¦ãªã„ãƒã‚¤ãƒ³ãƒˆï¼š0é™¤å¤– & æ—¥æœ¬èªã‚«ãƒ©ãƒ ï¼‰ ---
    # ignore_index=True ã‚’è¿½åŠ ã—ã¦ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®é‡è¤‡ã‚’è§£æ¶ˆã™ã‚‹
    merged_summary = pd.concat(all_summaries, ignore_index=True)

    plot_final_boxplots(merged_summary, agg_root / "model_eval")

    target_cols = ["è¨“ç·´RMSE", "äºˆæ¸¬RMSE", "RMSEæ¯”", "AIC", "æœ€å¤§å›ºæœ‰å€¤"]
    stats_list = []
    for col in target_cols:
        if col in merged_summary.columns:
            temp_series = merged_summary.copy()
            if col != "AIC": # AICä»¥å¤–ã¯0ã‚’å¤–ã™
                temp_series[col] = temp_series[col].replace(0, np.nan)

            res = temp_series.groupby("ãƒ¢ãƒ‡ãƒ«æ§‹æˆ")[col].agg([
                (f"{col}_å¹³å‡", "mean"),
                (f"{col}_æ¨™æº–åå·®", "std"),
                (f"{col}_æœ€å¤§", "max"),
                (f"{col}_æœ€å°", "min")
            ])
            stats_list.append(res)

    model_perf_detail = pd.concat(stats_list, axis=1)
    model_perf_detail["æœ‰åŠ¹è©¦è¡Œå›æ•°"] = merged_summary.groupby("ãƒ¢ãƒ‡ãƒ«æ§‹æˆ")["äºˆæ¸¬RMSE"].apply(lambda x: (x != 0).sum())
    model_perf_detail = model_perf_detail.round(4)
    model_perf_detail.to_csv(agg_root / "model_eval" / "å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é›†è¨ˆ_ãƒ¢ãƒ‡ãƒ«è©•ä¾¡_è©³ç´°ç‰ˆ.csv", encoding="utf-8-sig")

    # --- 3. PCAæ§‹é€ ã®é›†è¨ˆã¨ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆã“ã“ã‚‚å¿˜ã‚Œã¦ã¾ã›ã‚“ï¼ï¼‰ ---
    if pca_abs_details:
        full_pca_df = pd.concat(pca_abs_details)
        full_pca_df.to_csv(agg_root / "pca" / "å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é›†è¨ˆ_PCAæ§‹é€ _ç¬¦å·ã¤ãè©³ç´°.csv", encoding="utf-8-sig")
        # çµ¶å¯¾å€¤ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ä¿å­˜
        plot_signed_heatmaps(full_pca_df, agg_root / "heatmaps")
        plot_abs_heatmaps(full_pca_df, agg_root / "heatmaps")
    print(f"âœ… ã™ã¹ã¦ã®é›†è¨ˆãƒ»ç”»åƒä¿å­˜ï¼ˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã€ç®±ã²ã’å›³ã€çµ±è¨ˆè©³ç´°ï¼‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

def plot_final_boxplots(merged_summary, output_root):
    """
    ã‚ãªãŸãŒã€ãˆãˆæ„Ÿã˜ã€ã¨è¨€ã£ãŸã€è‰²ä»˜ãã®ç®±ã²ã’å›³ ï¼‹ çµ±è¨ˆé‡
    """
    output_root = Path(output_root)
    plot_df_base = merged_summary.copy().reset_index(drop=True)

    targets = ["è¨“ç·´RMSE", "äºˆæ¸¬RMSE", "RMSEæ¯”", "AIC", "æœ€å¤§å›ºæœ‰å€¤"]

    for col in targets:
        if col not in plot_df_base.columns: continue

        plt.figure(figsize=(14, 8))

        # 0é™¤å¤–ï¼ˆAICä»¥å¤–ï¼‰
        plot_data = plot_df_base[plot_df_base[col] != 0].copy() if col != "AIC" else plot_df_base.copy()
        if plot_data.empty: continue

        # å¹³å‡å€¤ãŒä½ã„é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ä¸¦ã¹ã‚‹
        order = plot_data.groupby("ãƒ¢ãƒ‡ãƒ«æ§‹æˆ")[col].mean().sort_values().index

        # 1. ã€ãƒ¡ã‚¤ãƒ³ã€‘è‰²ä»˜ãã®ç®±ã²ã’å›³ï¼ˆã“ã‚ŒãŒã€ãˆãˆæ„Ÿã˜ã€ã®æ­£ä½“ï¼‰
        sns.boxplot(data=plot_data, x="ãƒ¢ãƒ‡ãƒ«æ§‹æˆ", y=col, order=order,
                    palette="Set3", width=0.6, boxprops=dict(alpha=0.7))

        # 2. å¹³å‡å€¤(â—†)ã¨æ¨™æº–åå·®(èµ¤ç·š)ã‚’é‡ã­ã‚‹
        sns.pointplot(data=plot_data, x="ãƒ¢ãƒ‡ãƒ«æ§‹æˆ", y=col, order=order,
                      join=False, color="red", marker="D", scale=0.7,
                      errorbar="sd", capsize=.15, label="å¹³å‡ Â± æ¨™æº–åå·®")

        # 3. ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆè–„ã„ç‚¹ï¼‰ã¯ãƒã‚¤ã‚ºã«ãªã‚‹ã®ã§ã€ã“ã“ã§ã¯é™¤å¤–ã‹æ¥µè–„ã«
        # sns.stripplot(data=plot_data, x="ãƒ¢ãƒ‡ãƒ«æ§‹æˆ", y=col, order=order, color="black", alpha=0.1, jitter=True)

        plt.xticks(rotation=45, ha='right')
        plt.title(f"{col}ã®ç®±ã²ã’å›³", fontsize=14)
        plt.grid(axis='y', linestyle='--', alpha=0.4)
        plt.legend(loc='upper left')
        plt.tight_layout()

        plt.savefig(output_root / f"å…¨æœŸé–“_ç®±ã²ã’_{col}.png", dpi=300)
        plt.close()

def visualize_model_performance(csv_path):
    df = pd.read_csv(csv_path, index_col=0)
    output_dir = Path(csv_path).parent
    plt.style.use('ggplot')

    # --- AIC vs äºˆæ¸¬RMSE ã®æ•£å¸ƒå›³ã ã‘ä½œã‚‹ ---
    if "AIC_å¹³å‡" in df.columns and "äºˆæ¸¬RMSE_å¹³å‡" in df.columns:
        plt.figure(figsize=(10, 7))
        sns.scatterplot(
            data=df,
            x="AIC_å¹³å‡",
            y="äºˆæ¸¬RMSE_å¹³å‡",
            size="äºˆæ¸¬RMSE_æ¨™æº–åå·®" if "äºˆæ¸¬RMSE_æ¨™æº–åå·®" in df.columns else None,
            hue=df.index,
            sizes=(100, 1000),
            alpha=0.6
        )

        for i, txt in enumerate(df.index):
            plt.annotate(
                txt,
                (df["AIC_å¹³å‡"].iloc[i], df["äºˆæ¸¬RMSE_å¹³å‡"].iloc[i]),
                xytext=(5, 5),
                textcoords='offset points',
                fontsize=8
            )

        plt.title("ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•(AIC) vs äºˆæ¸¬ç²¾åº¦(RMSE)")
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(output_dir / "å…¨æœŸé–“_AIC_vs_RMSE_åˆ†å¸ƒ.png", dpi=300)
        plt.close()

    print(f"ğŸ’¾ æ•£å¸ƒå›³ã‚’ {output_dir} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

def contiguous_train_span(n_total, te_idx_list):
    """
    analyze_pca_details ãŒã€Œé€£ç¶šåŒºé–“ã€ã‚’å‰æã«ã—ã¦ã‚‹ã®ã§ã€
    trainã®å‰åŠ/å¾ŒåŠã®ã†ã¡é•·ã„æ–¹ã®é€£ç¶šåŒºé–“ [t_start, t_end) ã‚’è¿”ã™ã€‚
    """
    te_start = int(te_idx_list[0])
    te_end = int(te_idx_list[-1])

    pre_len = te_start
    post_len = n_total - (te_end + 1)

    if pre_len >= post_len:
        return 0, te_start
    else:
        return te_end + 1, n_total

SECTOR_GROUPS = {
    "PC1": SECTOR_COLS,
    "PC2": ["çŸ³æ²¹ãƒ»çŸ³ç‚­è£½å“", "é‰±æ¥­", "é›»æ°—ãƒ»ã‚¬ã‚¹æ¥­"],
    "PC3": ["é£Ÿæ–™å“"],
}

def fix_pca_sign_inplace(pca, feature_names, pc_cols, sector_groups, top_n=5):
    for pc_name in pc_cols:
        if pc_name not in sector_groups:
            continue

        # PC1 -> 0, PC2 -> 1 ...
        i = int(pc_name.replace("PC", "")) - 1

        target_sectors = sector_groups[pc_name]
        load = pd.Series(pca.components_[i], index=feature_names)

        group_load = load[load.index.isin(target_sectors)]
        if group_load.empty:
            continue

        top_idx = group_load.abs().sort_values(ascending=False).head(top_n).index
        sign_check = group_load.loc[top_idx].mean()

        if sign_check < 0:
            pca.components_[i] *= -1

from datetime import datetime

def _parse_window_start_date(window_path: Path):
    # Window_Test_20080630_20090331 -> 20080630
    parts = window_path.name.split("_")
    start_str = parts[2]
    return pd.to_datetime(start_str, format="%Y%m%d")

def _quarter_index(dt: pd.Timestamp):
    # å››åŠæœŸæœ«æƒ³å®šï¼ˆ3,6,9,12æœˆï¼‰ã€‚å¿µã®ãŸã‚ä¸€èˆ¬åŒ–ã€‚
    q = (dt.month - 1) // 3 + 1
    return dt.year * 4 + (q - 1)

def plot_connected_forecasts(window_root, save_root, df_raw, meta, model_subdir="PC1_DIFF", last_years=None):
    plt.style.use('default')
    window_root = Path(window_root)
    save_root = Path(save_root)
    save_root.mkdir(parents=True, exist_ok=True)

    # â˜…CSVã ã‘1æ®µæ·±ã„å ´æ‰€ã¸
    csv_root = save_root / "csv" / model_subdir
    csv_root.mkdir(parents=True, exist_ok=True)

    m_cols_transformed = list(meta.keys())

    window_dirs = [p for p in window_root.glob("Window_Test_*") if p.is_dir()]
    window_dirs = sorted(window_dirs, key=_parse_window_start_date)
    if not window_dirs:
        print("Window_Test_* ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    start0 = _parse_window_start_date(window_dirs[0])
    q0 = _quarter_index(start0)

    # phaseã”ã¨ã«äºˆæ¸¬ã‚’æ ¼ç´
    all_preds = {m: {p: [] for p in range(4)} for m in m_cols_transformed}
    rmse_list = {m: {p: [] for p in range(4)} for m in m_cols_transformed}

    for window_path in window_dirs:
        sub = window_path / model_subdir
        if not sub.exists():
            continue

        start_d = _parse_window_start_date(window_path)
        phase = (_quarter_index(start_d) - q0) % 4  # â†é–‹å§‹ä½ç½® mod4

        for m_key, m_info in meta.items():
            orig_name = m_info["orig"]
            target_csv = sub / f"äºˆæ¸¬å€¤_æ°´æº–ãƒ™ãƒ¼ã‚¹_{orig_name}.csv"
            if not target_csv.exists():
                continue

            pdf = pd.read_csv(target_csv, index_col=0, parse_dates=True)
            col_name = f"{orig_name}_Pred"
            if col_name not in pdf.columns:
                continue

            preds = pdf[col_name].dropna()
            if preds.empty:
                continue

            all_preds[m_key][phase].append(preds)

            # RMSE(ãã®windowå†…)ã‚‚ä¿å­˜
            actual = df_raw.loc[preds.index, orig_name].dropna()
            if not actual.empty:
                rmse = float(np.sqrt(np.mean((actual - preds.loc[actual.index])**2)))
                rmse_list[m_key][phase].append(rmse)

    # --- æç”»ï¼ˆå„å¤‰æ•°ã”ã¨ï¼‰ ---
    colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3']

    for m_col, m_info in meta.items():
        orig_name = m_info["orig"]
        if not any(len(all_preds[m_col][p]) > 0 for p in range(4)):
            continue

        fig, ax = plt.subplots(figsize=(14, 7), facecolor='white')
        ax.set_facecolor('white')

        # âœ… å®Ÿç¸¾ã¯ã€Œ1å›ã ã‘ã€ãƒ»å‰ã¨åŒã˜æ„Ÿã˜ï¼ˆç´°ã‚ + å°ã•ã„ä¸¸ï¼‰
        ax.plot(
            df_raw.index, df_raw[orig_name],
            color="#333333", lw=1.5,
            marker='o', markersize=3, alpha=0.7,
            label="å®Ÿç¸¾å€¤",
            zorder=2
        )

        rmse_summary = []
        for p in range(4):
            if not all_preds[m_col][p]:
                continue

            combined = pd.concat(all_preds[m_col][p]).sort_index()
            combined = combined[~combined.index.duplicated(keep='last')]

            avg_rmse = float(np.mean(rmse_list[m_col][p])) if rmse_list[m_col][p] else 0.0
            rmse_summary.append(f"P{p+1}:{avg_rmse:.3f}")

            # âœ… äºˆæ¸¬ã‚‚ã€Œå‰ã®æ„Ÿã˜ã€(ç´°ã‚ + å°ã•ã„ä¸¸) ã«æˆ»ã™
            ax.plot(
                combined.index, combined.values,
                color=colors[p],
                linestyle='--',
                marker='o', markersize=3,   # â† 4.5 â†’ 3
                lw=1.5, alpha=0.9,         # â† 2.6 â†’ 2.0
                label=f"ãƒ‘ã‚¿ãƒ¼ãƒ³{p+1} (RMSE: {avg_rmse:.3f})",
                zorder=5
            )

            combined.to_csv(csv_root / f"å…¨æœŸé–“é€£çµäºˆæ¸¬_{model_subdir}_{orig_name}_P{p+1}.csv", encoding="utf-8-sig")

        vals = [np.mean(v) for v in rmse_list[m_col].values() if len(v) > 0]
        total_avg = float(np.mean(vals)) if vals else 0.0

        ax.set_title(
            f"å…¨æœŸé–“é€£çµäºˆæ¸¬: {orig_name}\nã€RMSEã€‘{' / '.join(rmse_summary)} (å¹³å‡: {total_avg:.3f})",
            fontsize=14, pad=20
        )
        ax.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=True, facecolor='white')
        ax.grid(True, color='gray', linestyle=':', alpha=0.3)
        ax.set_ylabel("æ°´æº– (Level)")
        ax.set_xlabel("å¹´æœˆ")

        ax.xaxis.set_major_locator(mdates.YearLocator(base=1))
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
        ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[3,6,9,12]))
        ax.grid(True, which='major', axis='x', linestyle='--', alpha=0.25)
        ax.grid(True, which='minor', axis='x', linestyle=':',  alpha=0.10)

        plt.tight_layout()
        plt.savefig(save_root / f"å…¨æœŸé–“é€£çµäºˆæ¸¬_{model_subdir}_{orig_name}.png", dpi=200, facecolor='white')
        plt.close()

    print(f"âœ… PC1ã®ã¿ï¼ˆ{model_subdir}ï¼‰ã®å…¨æœŸé–“é€£çµäºˆæ¸¬ã‚’ä¿å­˜ã—ã¾ã—ãŸ")

def plot_exog_trends(csv_path, output_dir, target_var="GDP"):
    output_root = Path(output_dir)
    df = pd.read_csv(csv_path)

    sub_df = df[(df["Target_Variable"] == target_var) &
                (df["Model_Type"].str.contains("PC"))].copy()

    # --- ã€ã“ã“ãŒãƒã‚¤ãƒ³ãƒˆï¼šæ—¥ä»˜ã¸ã®å¤‰æ›ã€‘ ---
    # Windowåã®æœ«å°¾ï¼ˆãƒ†ã‚¹ãƒˆæœŸé–“ã®çµ‚äº†æ—¥ï¼‰ã‚’æ—¥ä»˜å‹ã«å¤‰æ›ã™ã‚‹
    sub_df["EndDateStr"] = sub_df["Window"].str.split("_").str[-1]
    sub_df["Date"] = pd.to_datetime(sub_df["EndDateStr"], format='%Y%m%d')
    sub_df = sub_df.sort_values("Date")

    models = sub_df["Model_Type"].unique()
    pc_cols = [c for c in sub_df.columns if c.startswith("PC")]

    fig, axes = plt.subplots(len(models), 1, figsize=(12, 4 * len(models)), sharex=True)
    if len(models) == 1: axes = [axes]

    for ax, m_type in zip(axes, models):
        m_data = sub_df[sub_df["Model_Type"] == m_type]
        active_pcs = [c for c in pc_cols if not m_data[c].isna().all()]

        for col in active_pcs:
            # Xè»¸ã«æ–‡å­—åˆ—ã§ã¯ãªãã€ŒDateã€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¸¡ã™
            ax.plot(m_data["Date"], m_data[col], marker='o', label=col, lw=2)

        ax.set_title(f"Target: {target_var} | Model: {m_type}", fontsize=12, fontweight='bold')
        ax.axhline(0.3, color='red', ls='--', alpha=0.6, label="LB: 0.3")
        ax.axhline(0, color='black', lw=1, alpha=0.3)

        # --- ã€ã“ã“ãŒãƒã‚¤ãƒ³ãƒˆï¼šXè»¸ã®ç›®ç››ã‚Šã‚’ç¶ºéº—ã«ã™ã‚‹ã€‘ ---
        ax.xaxis.set_major_locator(mdates.YearLocator()) # 1å¹´åˆ»ã¿
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y/%m')) # è¡¨ç¤ºå½¢å¼

        ax.set_ylabel("Coefficient")
        ax.legend(loc='upper right', fontsize=9)
        ax.grid(axis='both', alpha=0.2)

    plt.xticks(rotation=45)
    plt.tight_layout()

    save_path = output_root / f"ä¿‚æ•°æ¨ç§»_{target_var}_æ—¥ä»˜ä¿®æ­£ç‰ˆ.png"
    plt.savefig(save_path, dpi=200, facecolor='white')
    plt.close()
    print(f"âœ… æ—¥ä»˜ã‚’ä¿®æ­£ã—ã¦ä¿å­˜ã—ã¾ã—ãŸ: {save_path}")

# =========================================================
# 3. ãƒ¡ã‚¤ãƒ³å‡¦ç†
# =========================================================
def main():
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†ï¼ˆã“ã“ã¯å…±é€šï¼‰
    df_raw = pd.read_csv(CONFIG["data_path"], parse_dates=["Date"])
    df_raw = df_raw.set_index("Date").sort_index()
    sector_df, macro_df, common_idx, meta = prepare_aligned(df_raw)

    # --- Moving Window è¨­å®š ---
    test_len = CONFIG["test_steps"]
    n_total = len(common_idx)
    all_coefficients_records=[]

    # ãƒ†ã‚¹ãƒˆæœŸé–“ã‚’1æœŸãšã¤ãšã‚‰ã—ã¦å…¨çµ„ã¿åˆã‚ã›ã‚’å®Ÿè¡Œ
    for i in range(n_total - test_len + 1):
        # ãƒ†ã‚¹ãƒˆæœŸé–“ã¨è¨“ç·´æœŸé–“ï¼ˆãƒ†ã‚¹ãƒˆä»¥å¤–ã™ã¹ã¦ï¼‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        te_idx_list = np.arange(i, i + test_len)
        tr_idx_list = np.setdiff1d(np.arange(n_total), te_idx_list)

        # ãƒ•ã‚©ãƒ«ãƒ€åç”¨ã®æ—¥ä»˜å–å¾—
        d_start = common_idx[te_idx_list[0]]
        d_end = common_idx[te_idx_list[-1]]

        # ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®šï¼ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã”ã¨ã«ä½œæˆï¼‰
        root_dir = Path(CONFIG["output_dir"]) / f"Window_Test_{d_start:%Y%m%d}_{d_end:%Y%m%d}"
        root_dir.mkdir(parents=True, exist_ok=True)

        print(f"â–¶ï¸ å®Ÿè¡Œä¸­: {root_dir.name}")

        # =========================================================
        # PCA: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§Fitã—ã€å…¨æœŸé–“ã‚’Transformï¼ˆPCã¯3å›ºå®šï¼‰
        # =========================================================
        n_pcs = CONFIG["pc_max"]  # å¸¸ã«3

        sc_pca = StandardScaler()
        X_tr_pca = sc_pca.fit_transform(sector_df.iloc[tr_idx_list])

        pca = PCA(n_components=n_pcs).fit(X_tr_pca)
        pc_cols_orig = [f"PC{k+1}" for k in range(n_pcs)]  # ["PC1","PC2","PC3"]

        # â˜…è¿½åŠ ï¼šã“ã“ã§ç¬¦å·ã‚’ç¢ºå®šï¼ˆinplaceï¼‰
        fix_pca_sign_inplace(
            pca=pca,
            feature_names=sector_df.columns,     # â† PCAã«ä½¿ã£ãŸåˆ—é †ã¨ä¸€è‡´ã—ã¦ã‚‹ã“ã¨ãŒé‡è¦
            pc_cols=pc_cols_orig,
            sector_groups=SECTOR_GROUPS,
            top_n=5,
        )

        # å…¨æœŸé–“ã®PCã‚¹ã‚³ã‚¢ï¼ˆPC1..PC3ï¼‰
        full_pcs = pca.transform(sc_pca.transform(sector_df))
        pc_all = pd.DataFrame(full_pcs, index=sector_df.index, columns=pc_cols_orig)

        # å·®åˆ†ï¼ˆPC1..PC3ï¼‰
        pc_diff_all = pc_all.diff().dropna()
        pc_cols_diff = [f"PC{k+1}_DIFF" for k in range(n_pcs)]  # ["PC1_DIFF","PC2_DIFF","PC3_DIFF"]
        pc_diff_all.columns = pc_cols_diff

        # --- å…ƒã®åˆ†æãƒ¬ãƒãƒ¼ãƒˆ(analyze_pca_details)å®Ÿè¡Œ ---
        # å¼•æ•°ã¯Windowã«åˆã‚ã›ã¦èª¿æ•´
        analyze_pca_details(
            pca, sc_pca, sector_df, pc_cols_orig, root_dir,
            tr_idx_list=tr_idx_list, te_idx_list=te_idx_list
        )
        # ãƒã‚¯ãƒ­åŒæœŸ
        m_df_win = macro_df.loc[pc_diff_all.index]
        combinations = [[]] + [list(c) for r in range(1, n_pcs+1) for c in itertools.combinations(pc_cols_diff, r)]
        summary_list = []

        # combinations ä½œã£ãŸç›´å¾Œï¼ˆfor combo ã®å‰ï¼‰ã«ç§»å‹•
        print("macro cols:", list(m_df_win.columns))
        print("combo sample:", combinations[:5], " ... total:", len(combinations))

        # 1. ã™ã¹ã¦ã®åˆ¶ç´„ã‚’ã“ã“ã«é›†ç´„ (ç¯„å›²æŒ‡å®š or ç¬¦å·æŒ‡å®š)
        STRICT_CONSTRAINTS = {
            ("PC1", "GDP"): (0.3, 0.7),
            ("PC1", "NIKKEI"): (0.3, 0.7),
            ("PC1", "CPI"): (0.1, 0.5),
            ("PC1", "UNEMP_RATE"): (-0.7, -0.3),
            # ä»¥å‰ã® BASE_SIGN_CONSTRAINTS ç›¸å½“ã‚‚ã“ã“ã«æ›¸ã„ã¦ãŠã‘ã°ã‚¨ãƒ©ãƒ¼ã«ãªã‚Šã¾ã›ã‚“
            # ("*", "UNEMP_RATE"): (-np.inf, -0.3), # å¿…è¦ãªã‚‰è¿½åŠ 
        }

        for combo in combinations:
            m_name_display = ", ".join(combo) if combo else "ãƒã‚¯ãƒ­ã®ã¿"
            sub_dir = root_dir / ("_".join(combo) if combo else "BASE_VAR_ONLY")
            sub_dir.mkdir(parents=True, exist_ok=True)

            # --- è¨“ç·´ã¨ãƒ†ã‚¹ãƒˆã®åˆ‡ã‚Šå‡ºã— ---
            win_idx = m_df_win.index
            tr_pre_dates = common_idx[tr_idx_list[tr_idx_list < te_idx_list[0]]]
            tr_post_dates = common_idx[tr_idx_list[tr_idx_list > te_idx_list[-1]]]
            te_dates = common_idx[te_idx_list]

            # 1. éå»å´ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿
            tr_pre = pd.concat([m_df_win.loc[win_idx.isin(tr_pre_dates)],
                                pc_diff_all.loc[win_idx.isin(tr_pre_dates), combo]], axis=1)

            # 2. æœªæ¥å´ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿
            tr_post = pd.concat([m_df_win.loc[win_idx.isin(tr_post_dates)],
                                 pc_diff_all.loc[win_idx.isin(tr_post_dates), combo]], axis=1)

            # ã€é‡è¦ã€‘æœªæ¥å´ãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­1è¡Œã‚’NaNã«ã™ã‚‹ï¼ˆãƒ©ã‚°è¨ˆç®—ã§éå»å´ã¨ç¹‹ãŒã‚‹ã®ã‚’é˜²ããŸã‚ï¼‰
            if not tr_post.empty:
                tr_post.iloc[0, :] = np.nan

            # 3. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®çµåˆï¼ˆä¸­æŠœãã•ã‚ŒãŸç®‡æ‰€ã«NaNã®å£ãŒã§ãã‚‹ï¼‰
            tr_raw = pd.concat([tr_pre, tr_post])
            te_raw = pd.concat([m_df_win.loc[win_idx.isin(te_dates)],
                                pc_diff_all.loc[win_idx.isin(te_dates), combo]], axis=1)

            means, stds = tr_raw.mean(), tr_raw.std(ddof=0).replace(0, 1.0)
            tr_s, te_s = (tr_raw - means) / stds, (te_raw - means) / stds
            m_cols = list(m_df_win.columns)
            m_dim = len(m_cols)

            # ãƒ¢ãƒ‡ãƒ«æ¨å®š
            Y_tr, X_tr = make_design(tr_s[m_cols], tr_s[combo] if combo else None, CONFIG["p_lag"])

            if CONFIG.get("use_constraints", True):
                beta_list = []
                ridge_aug = np.sqrt(CONFIG["ridge"]) * np.eye(X_tr.shape[1])
                X_aug = np.vstack([X_tr, ridge_aug])

                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åãƒªã‚¹ãƒˆ
                param_names = ["CONST"] + [f"LAG_{m}" for m in m_cols] + list(combo)

                for j, target_m_col in enumerate(m_cols):
                    orig_target = meta[target_m_col]["orig"]
                    Y_aug = np.concatenate([Y_tr[:, j], np.zeros(X_tr.shape[1])])
                    lb = np.full(X_aug.shape[1], -np.inf)
                    ub = np.full(X_aug.shape[1], np.inf)

                    # --- ä¿®æ­£ç‰ˆï¼šãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰å¯¾å¿œã®åˆ¶ç´„é©ç”¨ãƒ­ã‚¸ãƒƒã‚¯ ---
                    if combo:
                        start_exog_idx = 1 + m_dim
                        for k, exog_pc_name in enumerate(combo):
                            orig_exog = exog_pc_name.split("_")[0]
                            target_idx = start_exog_idx + k

                            # å„ªå…ˆé †ä½ã‚’ã¤ã‘ã¦åˆ¶ç´„ã‚’æ¢ã™
                            # 1. å€‹åˆ¥æŒ‡å®š ("PC1", "GDP")
                            # 2. å¤–ç”Ÿå¤‰æ•°å…¨æŒ‡å®š ("*", "GDP")
                            # 3. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå…¨æŒ‡å®š ("PC1", "*")
                            # 4. å…¨æŒ‡å®š ("*", "*")
                            bound = (STRICT_CONSTRAINTS.get((orig_exog, orig_target)) or
                                    STRICT_CONSTRAINTS.get(("*", orig_target)) or
                                    STRICT_CONSTRAINTS.get((orig_exog, "*")) or
                                    STRICT_CONSTRAINTS.get(("*", "*")))

                            if bound:
                                lb[target_idx] = bound[0]
                                ub[target_idx] = bound[1]
                            # --- ã‚¨ãƒ©ãƒ¼ã®å…ƒã ã£ãŸ else ç¯€ (old_c) ã¯å‰Šé™¤ã—ã¾ã—ãŸ ---

                    res = lsq_linear(X_aug, Y_aug, bounds=(lb, ub), lsmr_tol='auto')
                    if not res.success:
                        # ä¿é™ºï¼šåˆ¶ç´„ã‚’ç„¡è¦–ã—ã¦é€šå¸¸ã®ridgeï¼ˆã¾ãŸã¯OLSï¼‰ã«è½ã¨ã™
                        res = lsq_linear(X_aug, Y_aug, bounds=(-np.inf, np.inf), lsmr_tol='auto')
                    beta_list.append(res.x)

                    record = {
                        "Window": root_dir.name,
                        "Model_Type": m_name_display,
                        "Target_Variable": orig_target,
                    }
                    # å„ä¿‚æ•°ã‚’ã‚«ãƒ©ãƒ ã¨ã—ã¦è¿½åŠ 
                    for name, val in zip(param_names, res.x):
                        record[name] = val

                    # mainã®å†’é ­ã§å®šç¾©ã—ãŸ all_coefficients_records ã«è¿½åŠ 
                    all_coefficients_records.append(record)

                Beta = np.array(beta_list).T
            else:
                # ä¿é™ºï¼šåˆ¶ç´„ã‚’åˆ‡ã£ãŸå ´åˆã§ã‚‚è½ã¡ãªã„ã‚ˆã†ã«OLS
                Beta = np.linalg.lstsq(X_tr, Y_tr, rcond=None)[0]

            # --- æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«æ¨å®šç›´å¾Œã‹ã‚‰ ---
            # æŒ‡æ¨™è¨ˆç®—
            tr_resid = Y_tr - X_tr @ Beta
            n_obs = len(Y_tr)
            tr_rmse = np.sqrt(np.mean(tr_resid**2))

            Y_te, X_te = make_design(te_s[m_cols], te_s[combo] if combo else None, CONFIG["p_lag"])
            te_rmse = np.sqrt(np.mean((Y_te - X_te @ Beta)**2)) if len(Y_te) > 0 else np.nan

            # --- AICè¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ã®å¼·åŒ–ç‰ˆ ---
            sigma_matrix = (tr_resid.T @ tr_resid) / n_obs
            sign, logdet = np.linalg.slogdet(sigma_matrix)

            if sign > 0:
                # é€šå¸¸ã®è¨ˆç®—
                aic_val = n_obs * logdet + 2 * Beta.size
            else:
                # è¡Œåˆ—å¼ãŒæ­£å¸¸ã«è¨ˆç®—ã§ããªã„å ´åˆã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
                # æ­£ã®å›ºæœ‰å€¤ã®ã¿ã‚’æŠ½å‡ºã—ã¦å¯¾æ•°å’Œã‚’ã¨ã‚‹
                eigs = np.linalg.eigvalsh(sigma_matrix)
                valid_eigs = eigs[eigs > 1e-15] # ã»ã¼ã‚¼ãƒ­ä»¥ä¸Šã®å›ºæœ‰å€¤ã®ã¿
                if len(valid_eigs) > 0:
                    aic_val = n_obs * np.sum(np.log(valid_eigs)) + 2 * Beta.size
                else:
                    aic_val = np.nan
            # -------------------------------

            A1 = Beta[1:1+m_dim, :].T
            max_eig_abs = np.max(np.abs(np.linalg.eigvals(A1)))

            summary_list.append({
                "ãƒ¢ãƒ‡ãƒ«æ§‹æˆ": m_name_display,
                "è¨“ç·´RMSE": round(tr_rmse, 4),
                "äºˆæ¸¬RMSE": round(te_rmse, 4) if not np.isnan(te_rmse) else np.nan,
                "RMSEæ¯”": round(te_rmse / tr_rmse, 2) if tr_rmse > 0 and not np.isnan(te_rmse) else np.nan,
                "AIC": round(aic_val, 2) if not np.isnan(aic_val) else np.nan,
                "æœ€å¤§å›ºæœ‰å€¤": round(max_eig_abs, 3)
            })

            # --- äºˆæ¸¬çµæœã®å¯è¦–åŒ– (é€æ¬¡äºˆæ¸¬ & å…¨ä½“è¡¨ç¤ºç‰ˆ) ---
            # 1. é€æ¬¡äºˆæ¸¬ (Dynamic Forecast) ã®å®Ÿè¡Œ
            y_te_pred_list = []
            if len(X_te) > 0:
                # æœ€åˆã®å…¥åŠ›ï¼ˆãƒ†ã‚¹ãƒˆæœŸé–“1ç‚¹ç›®ã®ãŸã‚ã®ãƒ©ã‚°ã‚’å«ã‚€ï¼‰
                curr_X = X_te[0:1, :]

                for t in range(CONFIG["test_steps"]):
                    # ç¾åœ¨ã®å…¥åŠ›ã§äºˆæ¸¬
                    pred_t = curr_X @ Beta  # shape: (1, m_dim)
                    y_te_pred_list.append(pred_t)

                    if t < CONFIG["test_steps"] - 1:
                        # æ¬¡ã®äºˆæ¸¬ã®ãŸã‚ã®å…¥åŠ›ã‚’æ§‹ç¯‰
                        # [å®šæ•°é …(1), ä»Šå›ã®äºˆæ¸¬å€¤(ãƒ©ã‚°1), å¤–ç”Ÿå¤‰æ•°(PC)ã®å®Ÿéš›å€¤]
                        next_lag_y = pred_t
                        # PCï¼ˆå¤–ç”Ÿå¤‰æ•°ï¼‰ã¯å®Ÿç¸¾ãŒã‚ã‚‹ç¯„å›²ã§ä½¿ã†ï¼ˆãªã‘ã‚Œã°0ã‹æœ€å¾Œã®å€¤ï¼‰
                        if combo:
                            next_exog = te_s[combo].iloc[t+1:t+2].values if (t+1) < len(te_s) else te_s[combo].iloc[-1:].values
                        else:
                            next_exog = np.empty((1, 0))

                        curr_X = np.concatenate([[[1.0]], next_lag_y, next_exog], axis=1)

                y_te_pred_scaled = np.vstack(y_te_pred_list)
            else:
                y_te_pred_scaled = None

            # i=0 (æœ€åˆæœŸ) ã‚’é™¤å¤–ã—ã€ã‹ã¤äºˆæ¸¬å€¤ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å®Ÿè¡Œ
            if y_te_pred_scaled is not None and i != 0:

                # 1. äºˆæ¸¬æœŸé–“ã®æ—¥ä»˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
                start_d_idx = np.where(common_idx == te_dates[0])[0][0]
                te_actual_dates = common_idx[start_d_idx : start_d_idx + CONFIG["test_steps"]]

                # å…¨æœŸé–“è¡¨ç¤ºï¼ˆ2008-
                # plot_start_idx = max(0, start_d_idx - 8)
                # plot_end_idx = min(len(common_idx), start_d_idx + CONFIG["test_steps"] + 4)
                # full_display_range = common_idx[plot_start_idx:plot_end_idx]    
                full_display_range = df_raw.index  # å®Ÿç¸¾ã¯ã“ã‚Œã§å…¨æœŸé–“

                for i_m, m_col in enumerate(m_cols):
                    orig_name = meta[m_col]["orig"]
                    meth = meta[m_col]["method"]

                    # 3. æ°´æº–å¾©å…ƒãƒ­ã‚¸ãƒƒã‚¯
                    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°è§£é™¤
                    pred_change = (y_te_pred_scaled[:, i_m] * stds[m_col]) + means[m_col]
                    # i != 0 ãªã®ã§å¿…ãšç›´å‰ã®å®Ÿç¸¾(ãƒ©ã‚°)
                    hist_before = df_raw.loc[df_raw.index < te_actual_dates[0], orig_name].dropna()
                    if hist_before.empty:
                        continue
                    last_actual_level = hist_before.iloc[-1]

                    pred_levels = []
                    curr_level = last_actual_level
                    for val in pred_change:
                        if meth == "LOGDIFF":
                            curr_level = curr_level * np.exp(val)
                        elif meth == "DIFF":
                            curr_level = curr_level + val
                        elif meth == "LEVEL":
                            curr_level = val
                        else:
                            curr_level = curr_level + val
                        pred_levels.append(curr_level)

                    # --- äºˆæ¸¬å€¤ã®è¨ˆç®—ãŒçµ‚ã‚ã£ãŸç›´å¾Œã«é…ç½® ---
                    p_df = pd.DataFrame(pred_levels, index=te_actual_dates, columns=[f"{orig_name}_Pred"])
                    # å¤‰æ•°åä»˜ãã§ä¿å­˜ã™ã‚‹ã“ã¨ã§ã€å¾Œã®é›†è¨ˆã‚’ç¢ºå®Ÿã«ã—ã¾ã™
                    p_df.to_csv(sub_dir / f"äºˆæ¸¬å€¤_æ°´æº–ãƒ™ãƒ¼ã‚¹_{orig_name}.csv", encoding="utf-8-sig")

                    # --- 4. ã‚°ãƒ©ãƒ•æç”» ---
                    plt.figure(figsize=(11, 5.5))

                    # å®Ÿç¸¾å€¤ (é»’) ã¨ äºˆæ¸¬å€¤ (èµ¤)
                    # å®Ÿç¸¾ï¼šè–„ã‚ã€å°ã•ã‚
                    plt.plot(
                        full_display_range, df_raw.loc[full_display_range, orig_name],
                        label="å®Ÿç¸¾å€¤",
                        color="#333333", lw=1.5,
                        marker='o', markersize=3, alpha=0.7,
                        zorder=2
                    )
                    # äºˆæ¸¬ï¼šå¤ªã‚ã€ç‚¹å¤§ãã‚ã€ç™½ç¸ã§åŸ‹ã‚‚ã‚Œé˜²æ­¢ã€å‰é¢
                    plt.plot(
                        te_actual_dates, pred_levels,
                        label="äºˆæ¸¬å€¤ (é€æ¬¡)",
                        color="red", lw=1.5, linestyle="--",
                        marker="o", markersize=3,
                        markerfacecolor="red", markeredgecolor="red",
                        alpha=0.9, zorder=5
                    )
                    # ãƒ†ã‚¹ãƒˆæœŸé–“ã®èƒŒæ™¯
                    plt.axvspan(te_actual_dates[0], te_actual_dates[-1], color='gray', alpha=0.1, label='äºˆæ¸¬å¯¾è±¡æœŸé–“')

                    ax = plt.gca()
                    # --- xè»¸ï¼ˆé•·æœŸå‘ã‘ï¼‰ï¼šå¹´ã ã‘ãƒ©ãƒ™ãƒ«ã€å››åŠæœŸã¯è£œåŠ© ---
                    ax.xaxis.set_major_locator(mdates.YearLocator(base=1))          # 1å¹´åˆ»ã¿
                    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))        # "2008" ã¿ãŸã„ã«å¹´ã ã‘

                    ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[3,6,9,12]))  # å››åŠæœŸæœ«ã¯è£œåŠ©ç›®ç››ã‚Š
                    ax.grid(True, which='major', axis='x', linestyle='--', alpha=0.25)
                    ax.grid(True, which='minor', axis='x', linestyle=':',  alpha=0.10)
                    ax.grid(True, which='major', axis='y', linestyle='--', alpha=0.25)

                    plt.xticks(rotation=0)  # å¹´ã ã‘ãªã‚‰å›è»¢ãªã—ã§OK
                    plt.title(f"ã€äºˆæ¸¬ã€‘{orig_name} : {m_name_display}", fontsize=14)
                    plt.xlabel("å¹´æœˆ", fontsize=12)
                    plt.ylabel("æ°´æº– (Level)", fontsize=12)
                    plt.legend(loc='best', frameon=True, shadow=True)
                    plt.tight_layout()

                    # ä¿å­˜
                    plt.savefig(sub_dir / f"PRED_{orig_name}.png", dpi=150)
                    plt.close()

            # --- å…ƒã®IRFãƒ—ãƒ­ãƒƒãƒˆ(å¤‰æ›´ãªã—) ---
            if CONFIG["do_irf"] and combo:
                B = Beta[1+m_dim:, :].T
                for j, pc_label in enumerate(combo):
                    pc_folder = sub_dir / f"Shock_{pc_label}"; pc_folder.mkdir(parents=True, exist_ok=True)
                    impact = np.zeros((13, m_dim)); impact[1] = B[:, j]
                    for h in range(2, 13): impact[h] = A1 @ impact[h-1]
                    for i_m, m_col in enumerate(m_cols):
                        orig_m, meth_m = meta[m_col]["orig"], meta[m_col]["method"]
                        base = BASE_LEVELS.get(orig_m, 100); imp_raw = impact[:, i_m] * stds[m_col]
                        vals = [base]
                        for s in range(1, 13):
                            if meth_m == "LOGDIFF":
                                vals.append(vals[-1] * np.exp(imp_raw[s]))
                            elif meth_m == "DIFF":
                                vals.append(vals[-1] + imp_raw[s])
                            elif meth_m == "LEVEL":
                                vals.append(base + imp_raw[s])  # LEVELã¯â€œåŸºæº–å€¤ï¼‹æ°´æº–ã‚·ãƒ§ãƒƒã‚¯â€
                            else:
                                vals.append(vals[-1] + imp_raw[s])
                        plt.figure(figsize=(6, 3.5))
                        ax = plt.gca()
                        ax.yaxis.get_major_formatter().set_useOffset(False)
                        ax.yaxis.get_major_formatter().set_scientific(False)
                        plt.plot(range(13), vals, color='k', lw=1)
                        plt.scatter(range(13), vals, marker='o', s=15, color='k', zorder=2)
                        plt.scatter(1, vals[1], facecolors='none', edgecolors='k', marker='o', s=100, lw=1, zorder=3)
                        plt.axvline(x=1, color='k', ls='--', lw=0.7, alpha=0.6)
                        plt.axhline(base, color='gray', ls=':', lw=0.8)
                        plt.title(f"{pc_label} â†’ {orig_m}")
                        plt.grid(alpha=0.15); plt.tight_layout()
                        plt.savefig(pc_folder / f"{orig_m}_å¿œç­”.png"); plt.close()

        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã”ã¨ã®ã‚µãƒãƒªãƒ¼ä¿å­˜
        pd.DataFrame(summary_list).sort_values("AIC").to_csv(root_dir / "ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚µãƒãƒªãƒ¼.csv", index=False, encoding="utf-8-sig")

    # --- å®Ÿè¡Œéƒ¨åˆ† ---
    agg_root = Path(CONFIG["output_dir"]) / "00_aggregate"
    for d in ["model_eval", "pca", "heatmaps", "forecasts", "coefficients"]:
        (agg_root / d).mkdir(parents=True, exist_ok=True)

    aggregate_results(Path(CONFIG["output_dir"]), agg_root=agg_root)

    csv_file = agg_root / "model_eval" / "å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é›†è¨ˆ_ãƒ¢ãƒ‡ãƒ«è©•ä¾¡_è©³ç´°ç‰ˆ.csv"
    if csv_file.exists():
        visualize_model_performance(csv_file)

    plot_connected_forecasts(CONFIG["output_dir"], agg_root / "forecasts", df_raw, meta, "PC1_DIFF")
    plot_connected_forecasts(CONFIG["output_dir"], agg_root / "forecasts", df_raw, meta, "BASE_VAR_ONLY")

    if all_coefficients_records:
        df_coef = pd.DataFrame(all_coefficients_records)
        coef_path = agg_root / "coefficients" / "all_model_coefficients.csv"
        df_coef.to_csv(coef_path, index=False, encoding="utf-8-sig")

        for t in ["GDP", "NIKKEI", "USD_JPY"]:
            plot_exog_trends(coef_path, agg_root / "coefficients", target_var=t)

    print(f"âœ… å…¨å·¥ç¨‹å®Œäº†ã€‚{agg_root} ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

if __name__ == "__main__":
    main()
