import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from pathlib import Path
import itertools
import warnings
import seaborn as sns
from scipy.optimize import lsq_linear
import matplotlib.dates as mdates
from matplotlib import patheffects as pe
import os
import platform
from matplotlib import font_manager as fm

warnings.simplefilter('ignore')

# =========================================================
# 1. è¨­å®šãƒ»å®šæ•°
# =========================================================
CONFIG = {
    "data_path": "./data/all_q_merged_new_tmp.csv",
    "output_dir": "./output_mw_new_tmp_5",
    "test_steps": 4,
    "pc_max": 3,
    "p_lag": 1,
    "ridge": 1.0,
    "do_irf": True,
    "verbose": False,
    "transform_overrides": {
        # "CPI": "LEVEL",   # å‰å¹´æ¯”%ã‚’ãã®ã¾ã¾ä½¿ã†ãªã‚‰
        # "CPI": "DIFF",    # å‰å¹´æ¯”%ã®å‰å¹´å·®ãªã‚‰
        # "CPI": "LOGDIFF", # CPIæŒ‡æ•°(>0)ãªã‚‰
        # "UNEMP_RATE": "LOGDIFF" 
    }
}

BASE_LEVELS = {
    "GDP": 500000, "NIKKEI": 20000, "USD_JPY": 150, "UNEMP_RATE": 3.0,
    "JGB_1Y": 0.0, "JGB_3Y": 0.0,
    # "JGB_10Y": 0.0,
    "CPI": 0.0, 
    # "TOPIX": 1500,
}

TARGET_MACRO = list(BASE_LEVELS.keys())

SECTOR_COLS = [
    "ã‚¬ãƒ©ã‚¹ãƒ»åœŸçŸ³è£½å“",
    "ã‚´ãƒ è£½å“",
    "é›»æ°—æ©Ÿå™¨",
    "é‡‘å±è£½å“",
    "ãã®ä»–è£½å“",
    "æ©Ÿæ¢°",
    "é£Ÿæ–™å“",
    "è¼¸é€ç”¨æ©Ÿå™¨",
    "åŒ–å­¦",
    "é›»æ°—ãƒ»ã‚¬ã‚¹æ¥­",
    "é‰±æ¥­",
    "é‰„é‹¼",
    "ç²¾å¯†æ©Ÿå™¨",
    "çŸ³æ²¹ãƒ»çŸ³ç‚­è£½å“",
]

# =========================================================
# 2. ãƒ­ã‚¸ãƒƒã‚¯é–¢æ•°
# =========================================================
def setup_japanese_font(prefer_path=None):
    import matplotlib.pyplot as plt
    import matplotlib as mpl
    from matplotlib import font_manager as fm
    from pathlib import Path
    import os, platform

    candidates = []
    if prefer_path:
        candidates.append(prefer_path)

    envp = os.environ.get("JP_FONT_PATH")
    if envp:
        candidates.append(envp)

    sys = platform.system()
    if sys == "Windows":
        candidates += [
            r"C:\Windows\Fonts\YuGothR.ttc",
            r"C:\Windows\Fonts\meiryo.ttc",
            r"C:\Windows\Fonts\msgothic.ttc",
        ]
    elif sys == "Darwin":
        candidates += [
            "/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc",
            "/System/Library/Fonts/Hiragino Sans GB.ttc",
        ]
    else:
        candidates += [
            "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
            "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
            "/usr/share/fonts/truetype/fonts-japanese-gothic.ttf",  # ç’°å¢ƒã«ã‚ˆã£ã¦
        ]

    for p in candidates:
        p = str(p)
        if p and Path(p).exists():
            fm.fontManager.addfont(p)
            fp = fm.FontProperties(fname=p)
            plt.rcParams["font.family"] = "sans-serif"
            plt.rcParams["font.sans-serif"] = [fp.get_name()]
            mpl.rcParams["axes.unicode_minus"] = False
            print(f"âœ… æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ: {fp.get_name()} ({p})")
            return

    # ã“ã“ã«æ¥ãŸã‚‰ã€Œãã‚‚ãã‚‚æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒç„¡ã„ã€
    mpl.rcParams["axes.unicode_minus"] = False
    print("âš ï¸ æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãšã€æ–‡å­—åŒ–ã‘ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚JP_FONT_PATH ã§ .ttf/.ttc ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")

def apply_style_and_jpfont(style_name="default", prefer_path=None):
    plt.style.use(style_name)          # â† å…ˆã«style
    setup_japanese_font(prefer_path)   # â† å¾Œã§ãƒ•ã‚©ãƒ³ãƒˆï¼ˆã“ã‚ŒãŒé‡è¦ï¼‰

def block_starts(s):
    # äºˆæ¸¬ãŒå§‹ã¾ã‚‹ç‚¹ï¼ˆNaNâ†’å€¤ã‚ã‚Šï¼‰
    return s.index[s.notna() & s.shift(1).isna()]

def make_full_pred(pred_by_h):  # {1:Series, 2:Series, 3:Series, 4:Series}
    full = pred_by_h[1].copy()
    for h in [2, 3, 4]:
        full = full.combine_first(pred_by_h[h])
    return full

def log(*args, **kwargs):
    if CONFIG.get("verbose", True):
        print(*args, **kwargs)

def smart_transform(series, name):
    overrides = CONFIG.get("transform_overrides", {})
    mode = overrides.get(name)

    if mode == "LEVEL":
        return series, "LEVEL"
    if mode == "DIFF":
        return series.diff(), "DIFF"
    if mode == "LOGDIFF":
        s = series.copy()
        s = s.where(s > 0, np.nan)  # 0ä»¥ä¸‹ã‚’NaN
        return np.log(s).diff(), "LOGDIFF"
    if mode == "PCTCHANGE":
        # pct_change ã¯ 0å‰²ã‚„ inf ãŒå‡ºã‚„ã™ã„ã®ã§ä¸€å¿œã‚±ã‚¢
        ts = series.pct_change().replace([np.inf, -np.inf], np.nan)
        return ts, "PCTCHANGE"

    # ---- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆç¾çŠ¶ã®è‡ªå‹•åˆ¤å®šï¼‰----
    if (series <= 0).any() or "JGB" in name or "UNEMP" in name:
        return series.diff(), "DIFF"
    return np.log(series).diff(), "LOGDIFF"

def prepare_aligned(df_raw):
    m_work = pd.DataFrame(index=df_raw.index)
    meta = {}
    for col in TARGET_MACRO:
        if col in df_raw.columns:
            ts, method = smart_transform(df_raw[col], col)
            m_work[f"{col}_{method}"] = ts
            meta[f"{col}_{method}"] = {"orig": col, "method": method}
    keep = [c for c in m_work.columns if c.startswith(("GDP_", "NIKKEI_", "USD_JPY_", "UNEMP_RATE_"))]
    m_df = m_work.dropna(subset=keep)
    s_raw = df_raw[[c for c in SECTOR_COLS if c in df_raw.columns]].interpolate(limit_direction='both')
    s_diff = s_raw.diff().dropna()
    common = s_diff.index.intersection(m_df.index)
    return s_diff.loc[common], m_df.loc[common], common, meta

def make_design(endog, exog, p):
    Y = endog.values
    X_parts = [np.ones((len(endog), 1))]
    for lag in range(1, p+1):
        X_parts.append(endog.shift(lag).values)
    if exog is not None and not exog.empty:
        X_parts.append(exog.values)
    X = np.concatenate(X_parts, axis=1)
    valid = ~np.isnan(X).any(axis=1) & ~np.isnan(Y).any(axis=1)

    idx_valid = endog.index[valid]
    return Y[valid], X[valid], idx_valid

def analyze_pca_details(pca, scaler, sector_df, pc_cols, root_dir, tr_idx_list, te_idx_list):
    pca_dir = root_dir / "pca_analysis"
    pca_dir.mkdir(exist_ok=True)

    idx = np.r_[tr_idx_list, te_idx_list]
    X_target = sector_df.iloc[idx].sort_index()
    X_scaled = scaler.transform(X_target)
    scores = pca.transform(X_scaled)
    score_df = pd.DataFrame(scores, index=X_target.index, columns=pc_cols)

    expl = pca.explained_variance_ratio_
    components = pca.components_.copy()

    score_df.to_csv(pca_dir / "ä¸»æˆåˆ†ã‚¹ã‚³ã‚¢_ç”Ÿãƒ‡ãƒ¼ã‚¿.csv", encoding="utf-8-sig")

    loadings = pd.DataFrame(components.T, index=sector_df.columns, columns=pc_cols)
    loadings.to_csv(pca_dir / "ã‚»ã‚¯ã‚¿ãƒ¼ã®è² è·é‡_ä¸€è¦§.csv", encoding="utf-8-sig")

    expl_df = pd.DataFrame(expl.reshape(1, -1), columns=pc_cols, index=["ExplainedVariance"])
    expl_df.to_csv(pca_dir / "ä¸»æˆåˆ†_å¯„ä¸ç‡.csv", encoding="utf-8-sig")

    # ã‚¹ã‚³ã‚¢æ¨ç§»ãƒ—ãƒ­ãƒƒãƒˆï¼ˆPC3ã¾ã§ï¼‰
    plt.figure(figsize=(12, 5))
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']
    for i, pc in enumerate(pc_cols[:3]):
        plt.plot(score_df.index, score_df[pc], label=f"{pc} (å¯„ä¸:{expl[i]*100:.1f}%)", color=colors[i], lw=2)
    plt.title("ä¸»æˆåˆ†ã‚¹ã‚³ã‚¢ã®æ¨ç§» (ã‚»ã‚¯ã‚¿ãƒ¼æ¨™æº–åŒ–å¾Œã®ç”Ÿå€¤)")

    # --- è»¸ãƒ©ãƒ™ãƒ«è¿½åŠ  ---
    plt.xlabel("æ—¥ä»˜")
    plt.ylabel("ä¸»æˆåˆ†ã‚¹ã‚³ã‚¢")

    plt.axhline(0, color='black', lw=1); plt.legend(loc='upper left', bbox_to_anchor=(1, 1)); plt.grid(axis='y', alpha=0.3); plt.tight_layout()
    plt.savefig(pca_dir / "PCA_ã‚¹ã‚³ã‚¢æ¨ç§»_ç”Ÿå€¤.png"); plt.close()

    # è² è·é‡ã®æ£’ã‚°ãƒ©ãƒ•ï¼ˆPC3ã¾ã§ï¼‰
    for i, pc in enumerate(pc_cols[:3]):
        plt.figure(figsize=(10, 5))
        loadings[pc].reindex(loadings[pc].abs().sort_values(ascending=False).index).head(10).plot(kind='bar', color=colors[i])
        plt.title(f"{pc} ã‚»ã‚¯ã‚¿ãƒ¼ã®è² è·é‡ (å¯„ä¸:{expl[i]*100:.1f}%)")

        # --- è»¸ãƒ©ãƒ™ãƒ«è¿½åŠ  ---
        plt.xlabel("ã‚»ã‚¯ã‚¿ãƒ¼")
        plt.ylabel("è² è·é‡")

        plt.axhline(0, color='black', lw=1); plt.xticks(rotation=45, ha='right'); plt.grid(axis='y', alpha=0.3); plt.tight_layout()
        plt.savefig(pca_dir / f"è² è·é‡_{pc}_TOP10.png"); plt.close()

    print(f"âœ… PCAåˆ†æå®Œäº†: {pca_dir}")

def plot_abs_heatmaps(full_pca_df, output_root):
    # PCã”ã¨ã«ãƒ«ãƒ¼ãƒ—ã—ã¦å¯è¦–åŒ–
    for pc in ["PC1", "PC2", "PC3"]:
        target_df = full_pca_df[full_pca_df["PC_Type"] == pc]
        if target_df.empty: continue

        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã—ã¦ã€ã‚»ã‚¯ã‚¿ãƒ¼åˆ—ã®ã¿æŠ½å‡ºï¼ˆæ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
        # æ–‡å­—åˆ—ã‚«ãƒ©ãƒ ã‚’é™¤å¤–ã—ã¦è»¢ç½®
        plot_data = target_df.drop(columns=["PC_Type", "Window", "Explained_Variance"]).T
        plot_data = plot_data.abs()
        plot_data.columns = target_df["Window"] # xè»¸ã‚’ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åã«

        plt.figure(figsize=(14, 9))

        # è² è·é‡ã®çµ¶å¯¾å€¤ãªã®ã§ã€0ã‹ã‚‰ã®å¼·å¼±ãŒã¯ã£ãã‚Šã™ã‚‹ "Reds" ãªã©ã‚’æ¡ç”¨
        # vmin=0, vmax=0.5 ãªã©ã§ã‚¹ã‚±ãƒ¼ãƒ«ã‚’å›ºå®šã™ã‚‹ã¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é–“ã®æ¯”è¼ƒãŒã—ã‚„ã™ããªã‚Šã¾ã™
        sns.heatmap(plot_data,
                    cmap="Reds",
                    annot=False,
                    cbar_kws={'label': f'{pc} è² è·é‡ï¼ˆçµ¶å¯¾å€¤ï¼‰'},
                    vmin=0,
                    vmax=max(0.5, plot_data.max().max()))

        # å¯„ä¸ç‡ã‚’å–å¾—
        mean_var = target_df["Explained_Variance"].mean() * 100
        plt.title(f"{pc} æ§‹æˆã‚»ã‚¯ã‚¿ãƒ¼ã®è² è·é‡æ¨ç§»ï¼ˆçµ¶å¯¾å€¤ï¼‰\n[å¹³å‡å¯„ä¸ç‡: {mean_var:.1f}%]", fontsize=14)
        plt.xlabel("ãƒ†ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ (Window)", fontsize=12)
        plt.ylabel("ã‚»ã‚¯ã‚¿ãƒ¼", fontsize=12)

        plt.tight_layout()
        plt.savefig(output_root / f"å…¨æœŸé–“_{pc}_æ§‹é€ æ¨ç§»_çµ¶å¯¾å€¤.png")
        plt.close()

def plot_signed_heatmaps(full_pca_df, output_root):
    for pc in ["PC1", "PC2", "PC3"]:
        target_df = full_pca_df[full_pca_df["PC_Type"] == pc]
        if target_df.empty:
            continue

        plot_data = target_df.drop(columns=["PC_Type", "Window", "Explained_Variance"]).T
        plot_data.columns = target_df["Window"]

        vmax = np.nanmax(np.abs(plot_data.values))
        vmax = max(0.5, float(vmax))  # è¦‹ã‚„ã™ã•ã®ä¸‹é™

        plt.figure(figsize=(14, 9))
        sns.heatmap(
            plot_data,
            cmap="RdBu_r",
            center=0,
            vmin=-vmax,
            vmax=vmax,
            annot=False,
            cbar_kws={"label": f"{pc} è² è·é‡ï¼ˆç¬¦å·ã¤ãï¼‰"},
        )

        mean_var = target_df["Explained_Variance"].mean() * 100
        plt.title(f"{pc} æ§‹æˆã‚»ã‚¯ã‚¿ãƒ¼ã®è² è·é‡æ¨ç§»ï¼ˆç¬¦å·ã¤ãï¼‰\n[å¹³å‡å¯„ä¸ç‡: {mean_var:.1f}%]", fontsize=14)
        plt.xlabel("ãƒ†ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ (Window)")
        plt.ylabel("ã‚»ã‚¯ã‚¿ãƒ¼")
        plt.tight_layout()
        plt.savefig(output_root / f"å…¨æœŸé–“_{pc}_æ§‹é€ æ¨ç§»_ç¬¦å·ã¤ã.png")
        plt.close()

def aggregate_results(output_root, agg_root=None):
    output_root = Path(output_root)
    agg_root = Path(agg_root) if agg_root is not None else output_root
    all_summaries = []
    pca_abs_details = []

    # 1. å„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’å·¡å›ã—ã¦ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’åé›†
    for window_path in sorted(output_root.glob("Window_Test_*")):
        window_name = window_path.name.replace("Window_Test_", "")

        # ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚µãƒãƒªãƒ¼
        summary_file = window_path / "ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚µãƒãƒªãƒ¼.csv"
        if summary_file.exists():
            df = pd.read_csv(summary_file)
            df["Test_Window"] = window_name
            all_summaries.append(df)

        # PCAè² è·é‡ï¼ˆçµ¶å¯¾å€¤ï¼‰
        loadings_file = window_path / "pca_analysis" / "ã‚»ã‚¯ã‚¿ãƒ¼ã®è² è·é‡_ä¸€è¦§.csv"
        variance_file = window_path / "pca_analysis" / "ä¸»æˆåˆ†_å¯„ä¸ç‡.csv"
        if loadings_file.exists() and variance_file.exists():
            ld_df = pd.read_csv(loadings_file, index_col=0)
            vr_df = pd.read_csv(variance_file, index_col=0)
            for pc in ["PC1", "PC2", "PC3"]:
                if pc in ld_df.columns:
                    row = ld_df[pc].to_frame().T   # â† abs() ã‚’æ¶ˆã™
                    row.index = [f"{window_name}_{pc}"]
                    row.insert(0, "Window", window_name)
                    row.insert(1, "PC_Type", pc)
                    row.insert(2, "Explained_Variance", vr_df.at["ExplainedVariance", pc])
                    pca_abs_details.append(row)

    if not all_summaries:
        return

    # --- 2. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã®è©³ç´°çµ±è¨ˆï¼ˆå¿˜ã‚Œã¦ãªã„ãƒã‚¤ãƒ³ãƒˆï¼š0é™¤å¤– & æ—¥æœ¬èªã‚«ãƒ©ãƒ ï¼‰ ---
    # ignore_index=True ã‚’è¿½åŠ ã—ã¦ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®é‡è¤‡ã‚’è§£æ¶ˆã™ã‚‹
    merged_summary = pd.concat(all_summaries, ignore_index=True)

    plot_final_boxplots(merged_summary, agg_root / "model_eval")

    target_cols = ["è¨“ç·´RMSE", "äºˆæ¸¬RMSE", "RMSEæ¯”", "AIC", "æœ€å¤§å›ºæœ‰å€¤"]
    stats_list = []
    for col in target_cols:
        if col in merged_summary.columns:
            temp_series = merged_summary.copy()
            if col != "AIC": # AICä»¥å¤–ã¯0ã‚’å¤–ã™
                temp_series[col] = temp_series[col].replace(0, np.nan)

            res = temp_series.groupby("ãƒ¢ãƒ‡ãƒ«æ§‹æˆ")[col].agg([
                (f"{col}_å¹³å‡", "mean"),
                (f"{col}_æ¨™æº–åå·®", "std"),
                (f"{col}_æœ€å¤§", "max"),
                (f"{col}_æœ€å°", "min")
            ])
            stats_list.append(res)

    model_perf_detail = pd.concat(stats_list, axis=1)
    model_perf_detail["æœ‰åŠ¹è©¦è¡Œå›æ•°"] = merged_summary.groupby("ãƒ¢ãƒ‡ãƒ«æ§‹æˆ")["äºˆæ¸¬RMSE"].apply(lambda x: (x != 0).sum())
    model_perf_detail = model_perf_detail.round(4)
    model_perf_detail.to_csv(agg_root / "model_eval" / "å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é›†è¨ˆ_ãƒ¢ãƒ‡ãƒ«è©•ä¾¡_è©³ç´°ç‰ˆ.csv", encoding="utf-8-sig")

    # --- 3. PCAæ§‹é€ ã®é›†è¨ˆã¨ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆã“ã“ã‚‚å¿˜ã‚Œã¦ã¾ã›ã‚“ï¼ï¼‰ ---
    if pca_abs_details:
        full_pca_df = pd.concat(pca_abs_details)
        full_pca_df.to_csv(agg_root / "pca" / "å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é›†è¨ˆ_PCAæ§‹é€ _ç¬¦å·ã¤ãè©³ç´°.csv", encoding="utf-8-sig")
        # çµ¶å¯¾å€¤ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ä¿å­˜
        plot_signed_heatmaps(full_pca_df, agg_root / "heatmaps")
        plot_abs_heatmaps(full_pca_df, agg_root / "heatmaps")
    print(f"âœ… ã™ã¹ã¦ã®é›†è¨ˆãƒ»ç”»åƒä¿å­˜ï¼ˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã€ç®±ã²ã’å›³ã€çµ±è¨ˆè©³ç´°ï¼‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

def plot_final_boxplots(merged_summary, output_root):
    """
    ã‚ãªãŸãŒã€ãˆãˆæ„Ÿã˜ã€ã¨è¨€ã£ãŸã€è‰²ä»˜ãã®ç®±ã²ã’å›³ ï¼‹ çµ±è¨ˆé‡
    """
    output_root = Path(output_root)
    plot_df_base = merged_summary.copy().reset_index(drop=True)

    targets = ["è¨“ç·´RMSE", "äºˆæ¸¬RMSE", "RMSEæ¯”", "AIC", "æœ€å¤§å›ºæœ‰å€¤"]

    for col in targets:
        if col not in plot_df_base.columns: continue

        plt.figure(figsize=(14, 8))

        # 0é™¤å¤–ï¼ˆAICä»¥å¤–ï¼‰
        plot_data = plot_df_base[plot_df_base[col] != 0].copy() if col != "AIC" else plot_df_base.copy()
        if plot_data.empty: continue

        # å¹³å‡å€¤ãŒä½ã„é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ä¸¦ã¹ã‚‹
        order = plot_data.groupby("ãƒ¢ãƒ‡ãƒ«æ§‹æˆ")[col].mean().sort_values().index

        # 1. ã€ãƒ¡ã‚¤ãƒ³ã€‘è‰²ä»˜ãã®ç®±ã²ã’å›³ï¼ˆã“ã‚ŒãŒã€ãˆãˆæ„Ÿã˜ã€ã®æ­£ä½“ï¼‰
        sns.boxplot(data=plot_data, x="ãƒ¢ãƒ‡ãƒ«æ§‹æˆ", y=col, order=order,
                    palette="Set3", width=0.6, boxprops=dict(alpha=0.7))

        # 2. å¹³å‡å€¤(â—†)ã¨æ¨™æº–åå·®(èµ¤ç·š)ã‚’é‡ã­ã‚‹
        sns.pointplot(data=plot_data, x="ãƒ¢ãƒ‡ãƒ«æ§‹æˆ", y=col, order=order,
                      join=False, color="red", marker="D", scale=0.7,
                      errorbar="sd", capsize=.15, label="å¹³å‡ Â± æ¨™æº–åå·®")

        # 3. ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆè–„ã„ç‚¹ï¼‰ã¯ãƒã‚¤ã‚ºã«ãªã‚‹ã®ã§ã€ã“ã“ã§ã¯é™¤å¤–ã‹æ¥µè–„ã«
        # sns.stripplot(data=plot_data, x="ãƒ¢ãƒ‡ãƒ«æ§‹æˆ", y=col, order=order, color="black", alpha=0.1, jitter=True)

        plt.xticks(rotation=45, ha='right')
        plt.title(f"{col}ã®ç®±ã²ã’å›³", fontsize=14)
        plt.grid(axis='y', linestyle='--', alpha=0.4)
        plt.legend(loc='upper left')
        plt.tight_layout()

        plt.savefig(output_root / f"å…¨æœŸé–“_ç®±ã²ã’_{col}.png", dpi=300)
        plt.close()

def visualize_model_performance(csv_path):
    df = pd.read_csv(csv_path, index_col=0)
    output_dir = Path(csv_path).parent

    # ã“ã“ã‚’ ggplot ã‹ã‚‰æˆ»ã™ï¼ˆç°èƒŒæ™¯ã®å…ƒï¼‰
    apply_style_and_jpfont("default")

    if "AIC_å¹³å‡" in df.columns and "äºˆæ¸¬RMSE_å¹³å‡" in df.columns:
        fig, ax = plt.subplots(figsize=(10, 7), facecolor="white")
        ax.set_facecolor("white")

        # seaborn scatter
        sc = sns.scatterplot(
            data=df,
            x="AIC_å¹³å‡",
            y="äºˆæ¸¬RMSE_å¹³å‡",
            size="äºˆæ¸¬RMSE_æ¨™æº–åå·®" if "äºˆæ¸¬RMSE_æ¨™æº–åå·®" in df.columns else None,
            hue=df.index,              # ãƒ¢ãƒ‡ãƒ«æ§‹æˆ
            sizes=(100, 1000),
            alpha=0.75,
            ax=ax
        )

        # ç‚¹ãƒ©ãƒ™ãƒ«ï¼ˆå¿…è¦ãªã‚‰ï¼‰
        for i, txt in enumerate(df.index):
            ax.annotate(
                txt,
                (df["AIC_å¹³å‡"].iloc[i], df["äºˆæ¸¬RMSE_å¹³å‡"].iloc[i]),
                xytext=(5, 5),
                textcoords="offset points",
                fontsize=8
            )

        ax.set_title("ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•(AIC) vs äºˆæ¸¬ç²¾åº¦(RMSE)")
        ax.grid(True, alpha=0.25)
        ax.set_xlabel("AIC_å¹³å‡")
        ax.set_ylabel("äºˆæ¸¬RMSE_å¹³å‡")

        # -----------------------------
        # å‡¡ä¾‹ã‚’ã€Œè‰²(hue)ã€ã¨ã€Œã‚µã‚¤ã‚º(size)ã€ã§åˆ†é›¢ã—ã¦å³å¤–ã¸
        # -----------------------------
        handles, labels = ax.get_legend_handles_labels()

        # seaborn ã®å‡¡ä¾‹ã¯ [hueã‚¿ã‚¤ãƒˆãƒ«, hueé …ç›®..., sizeã‚¿ã‚¤ãƒˆãƒ«, sizeé …ç›®...] ã®é †ã«ãªã‚ŠãŒã¡
        # ã“ã“ã‚’å®‰å®šã«åˆ†å‰²ã™ã‚‹ãŸã‚ã€ã‚¿ã‚¤ãƒˆãƒ«æ–‡å­—ã§åŒºåˆ‡ã‚‹
        hue_title = "ãƒ¢ãƒ‡ãƒ«æ§‹æˆ"
        size_title = "äºˆæ¸¬RMSE_æ¨™æº–åå·®"

        def split_legend(handles, labels, title_a, title_b):
            # title_a ãŒå‡ºã¦ãã‚‹ä½ç½®ã€title_b ãŒå‡ºã¦ãã‚‹ä½ç½®ã‚’æ¢ã™
            ia = labels.index(title_a) if title_a in labels else None
            ib = labels.index(title_b) if title_b in labels else None
            if ia is None:
                return None
            if ib is None:
                # hue ã—ã‹ç„¡ã„
                return (handles[ia+1:], labels[ia+1:], None, None)

            hue_h = handles[ia+1:ib]
            hue_l = labels[ia+1:ib]
            size_h = handles[ib+1:]
            size_l = labels[ib+1:]
            return (hue_h, hue_l, size_h, size_l)

        sp = split_legend(handles, labels, hue_title, size_title)

        # æ—¢å­˜ã®ä¸€ä½“åŒ–å‡¡ä¾‹ã‚’æ¶ˆã™
        leg0 = ax.get_legend()
        if leg0 is not None:
            leg0.remove()

        # hue å‡¡ä¾‹ï¼ˆä¸Šï¼‰
        if sp is not None:
            hue_h, hue_l, size_h, size_l = sp

            leg1 = ax.legend(
                hue_h, hue_l,
                title=hue_title,
                loc="upper left",
                bbox_to_anchor=(1.02, 1.00),
                borderaxespad=0.,
                frameon=True
            )
            ax.add_artist(leg1)

            # size å‡¡ä¾‹ï¼ˆä¸‹ï¼‰
            if size_h is not None and len(size_h) > 0:
                ax.legend(
                    size_h, size_l,
                    title=size_title,
                    loc="upper left",
                    bbox_to_anchor=(1.02, 0.55),
                    borderaxespad=0.,
                    frameon=True
                )
        else:
            # ä¸‡ä¸€åˆ†å‰²ã§ããªã„æ™‚ã¯ã€å³å¤–ã¸ã ã‘å‡ºã™
            ax.legend(loc="upper left", bbox_to_anchor=(1.02, 1.0), frameon=True)

        # å³ã«å‡¡ä¾‹ã‚¹ãƒšãƒ¼ã‚¹ç¢ºä¿
        fig.tight_layout(rect=[0, 0, 0.78, 1])

        save_path = output_dir / "å…¨æœŸé–“_AIC_vs_RMSE_åˆ†å¸ƒ.png"
        fig.savefig(save_path, dpi=300, facecolor="white")
        plt.close(fig)

    print(f"ğŸ’¾ æ•£å¸ƒå›³ã‚’ {output_dir} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

def contiguous_train_span(n_total, te_idx_list):
    """
    analyze_pca_details ãŒã€Œé€£ç¶šåŒºé–“ã€ã‚’å‰æã«ã—ã¦ã‚‹ã®ã§ã€
    trainã®å‰åŠ/å¾ŒåŠã®ã†ã¡é•·ã„æ–¹ã®é€£ç¶šåŒºé–“ [t_start, t_end) ã‚’è¿”ã™ã€‚
    """
    te_start = int(te_idx_list[0])
    te_end = int(te_idx_list[-1])

    pre_len = te_start
    post_len = n_total - (te_end + 1)

    if pre_len >= post_len:
        return 0, te_start
    else:
        return te_end + 1, n_total

SECTOR_GROUPS = {
    "PC1": SECTOR_COLS,
    "PC2": ["çŸ³æ²¹ãƒ»çŸ³ç‚­è£½å“", "é‰±æ¥­", "é›»æ°—ãƒ»ã‚¬ã‚¹æ¥­"],
    "PC3": ["é£Ÿæ–™å“"],
}

def fix_pca_sign_inplace(pca, feature_names, pc_cols, sector_groups, top_n=5):
    for pc_name in pc_cols:
        if pc_name not in sector_groups:
            continue

        # PC1 -> 0, PC2 -> 1 ...
        i = int(pc_name.replace("PC", "")) - 1

        target_sectors = sector_groups[pc_name]
        load = pd.Series(pca.components_[i], index=feature_names)

        group_load = load[load.index.isin(target_sectors)]
        if group_load.empty:
            continue

        top_idx = group_load.abs().sort_values(ascending=False).head(top_n).index
        sign_check = group_load.loc[top_idx].mean()

        if sign_check < 0:
            pca.components_[i] *= -1

def _parse_window_start_date(window_path: Path):
    # Window_Test_20080630_20090331 -> 20080630
    parts = window_path.name.split("_")
    start_str = parts[2]
    return pd.to_datetime(start_str, format="%Y%m%d")

def _quarter_index(dt: pd.Timestamp):
    # å››åŠæœŸæœ«æƒ³å®šï¼ˆ3,6,9,12æœˆï¼‰ã€‚å¿µã®ãŸã‚ä¸€èˆ¬åŒ–ã€‚
    q = (dt.month - 1) // 3 + 1
    return dt.year * 4 + (q - 1)

def plot_connected_forecasts(window_root, save_root, df_raw, meta, model_subdir="PC1_DIFF", last_years=None):
    apply_style_and_jpfont("default")
    window_root = Path(window_root)
    save_root = Path(save_root)
    save_root.mkdir(parents=True, exist_ok=True)

    # CSVå‡ºåŠ›å…ˆï¼ˆãƒ¢ãƒ‡ãƒ«åˆ¥ï¼‰
    csv_root = save_root / "csv" / model_subdir
    csv_root.mkdir(parents=True, exist_ok=True)

    # è¿½åŠ ï¼šç”»åƒå‡ºåŠ›å…ˆï¼ˆãƒ¢ãƒ‡ãƒ«åˆ¥ï¼‰
    fig_root = save_root / "fig" / model_subdir
    fig_root.mkdir(parents=True, exist_ok=True)

    m_cols_transformed = list(meta.keys())

    window_dirs = [p for p in window_root.glob("Window_Test_*") if p.is_dir()]
    window_dirs = sorted(window_dirs, key=_parse_window_start_date)
    if not window_dirs:
        print("Window_Test_* ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    start0 = _parse_window_start_date(window_dirs[0])
    q0 = _quarter_index(start0)

    # phaseã”ã¨ã«äºˆæ¸¬ã‚’æ ¼ç´
    all_preds = {m: {p: [] for p in range(4)} for m in m_cols_transformed}
    rmse_list = {m: {p: [] for p in range(4)} for m in m_cols_transformed}

    for window_path in window_dirs:
        sub = window_path / model_subdir
        if not sub.exists():
            continue

        start_d = _parse_window_start_date(window_path)
        phase = (_quarter_index(start_d) - q0) % 4  # é–‹å§‹ä½ç½® mod4

        for m_key, m_info in meta.items():
            orig_name = m_info["orig"]
            target_csv = sub / f"äºˆæ¸¬å€¤_æ°´æº–ãƒ™ãƒ¼ã‚¹_{orig_name}.csv"
            if not target_csv.exists():
                continue

            pdf = pd.read_csv(target_csv, index_col=0, parse_dates=True)
            col_name = f"{orig_name}_Pred"
            if col_name not in pdf.columns:
                continue

            preds = pdf[col_name].dropna()
            if preds.empty:
                continue

            all_preds[m_key][phase].append(preds)

            # RMSE(ãã®windowå†…)ã‚‚ä¿å­˜
            actual = df_raw.loc[preds.index, orig_name].dropna()
            if not actual.empty:
                rmse = float(np.sqrt(np.mean((actual - preds.loc[actual.index])**2)))
                rmse_list[m_key][phase].append(rmse)

    # --- æç”»ï¼†ä¿å­˜ï¼ˆå„å¤‰æ•°ã”ã¨ï¼‰ ---
    for m_col, m_info in meta.items():
        orig_name = m_info["orig"]

        # 4ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã©ã‚Œã‚‚ç„¡ã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—
        if not any(len(all_preds[m_col][p]) > 0 for p in range(4)):
            continue

        # å®Ÿç¸¾
        actual = df_raw[orig_name].copy()
        full_index = actual.index

        # phase(P1..P4)ã®é€£çµSeriesã‚’ä½œã‚‹ â†’ full_index ã« reindex ã—ã¦ NaN ã§åˆ‡ã‚Œç›®ã‚’ä½œã‚‹
        pred_by_h = {}
        rmse_summary = []

        for p in range(4):
            if all_preds[m_col][p]:
                combined = pd.concat(all_preds[m_col][p]).sort_index()
                combined = combined[~combined.index.duplicated(keep='last')]
                combined = combined.reindex(full_index)  # â˜…ã‚®ãƒ£ãƒƒãƒ—ã¯NaNã«ãªã‚‹ï¼ˆç·šãŒå‹æ‰‹ã«åˆ‡ã‚Œã‚‹ï¼‰
            else:
                combined = pd.Series(np.nan, index=full_index)

            pred_by_h[p+1] = combined

            avg_rmse = float(np.mean(rmse_list[m_col][p])) if rmse_list[m_col][p] else 0.0
            rmse_summary.append(f"P{p+1}:{avg_rmse:.3f}")

            # æ—¢å­˜ã¨åŒã˜ãã€å„è‰²CSVã‚‚ä¿å­˜ï¼ˆdropnaã—ã¦ä¿å­˜ï¼‰
            out_p_csv = csv_root / f"å…¨æœŸé–“é€£çµäºˆæ¸¬_{model_subdir}_{orig_name}_P{p+1}.csv"
            combined.dropna().to_frame(f"{orig_name}_Pred").to_csv(out_p_csv, encoding="utf-8-sig")

        # å…¨æœŸé–“äºˆæ¸¬ï¼ˆä¸€æœ¬ï¼‰ã‚’ä½œã‚‹ï¼ˆP1å„ªå…ˆâ†’P2â†’P3â†’P4ã§ç©´åŸ‹ã‚ï¼‰
        full_pred = make_full_pred(pred_by_h)

        # äºˆæ¸¬ãŒå­˜åœ¨ã™ã‚‹æœŸé–“ã ã‘ç°è‰²ã‚·ã‚§ãƒ¼ãƒ‰ï¼ˆä»»æ„ï¼‰
        shade = None
        if full_pred.notna().any():
            shade = (full_pred.first_valid_index(), full_pred.last_valid_index())

        # --- (1) 4è‰²ã¾ã¨ã‚ + å…¨æœŸé–“ä¸€æœ¬ + é–‹å§‹ç‚¹ä¸¸ ---
        title = f"å…¨æœŸé–“é€£çµäºˆæ¸¬: {orig_name}\nã€RMSEã€‘{' / '.join(rmse_summary)}"
        out_png = fig_root / f"å…¨æœŸé–“é€£çµäºˆæ¸¬_{model_subdir}_{orig_name}_ALL.png"
        plot_forecasts_all(actual, pred_by_h, out_png, title, shade=shade)

        # --- (2) è‰²ã”ã¨ã«1æšãšã¤ï¼ˆé–‹å§‹ç‚¹ä¸¸å…¥ã‚Šï¼‰ ---
        each_dir = fig_root / f"{orig_name}_each"
        base_title = f"å…¨æœŸé–“é€£çµäºˆæ¸¬: {orig_name} | {model_subdir}"
        plot_forecast_each(actual, pred_by_h, each_dir, base_title, shade=shade)

        # --- (3) CSVã‚‚Pred_Fullä»˜ãã§ä¿å­˜ ---
        out_all_csv = csv_root / f"å…¨æœŸé–“é€£çµäºˆæ¸¬_{model_subdir}_{orig_name}_ALL.csv"
        export_forecast_csv(actual, pred_by_h, out_all_csv)

    print(f"âœ… å…¨æœŸé–“é€£çµäºˆæ¸¬ï¼ˆALL + å„P + é–‹å§‹ç‚¹ä¸¸ + Pred_Full CSVï¼‰ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {fig_root}")

def plot_exog_trends(csv_path, output_dir, target_var="GDP"):
    output_root = Path(output_dir)
    df = pd.read_csv(csv_path)

    sub_df = df[(df["Target_Variable"] == target_var) &
                (df["Model_Type"].str.contains("PC"))].copy()

    # --- ã€ã“ã“ãŒãƒã‚¤ãƒ³ãƒˆï¼šæ—¥ä»˜ã¸ã®å¤‰æ›ã€‘ ---
    # Windowåã®æœ«å°¾ï¼ˆãƒ†ã‚¹ãƒˆæœŸé–“ã®çµ‚äº†æ—¥ï¼‰ã‚’æ—¥ä»˜å‹ã«å¤‰æ›ã™ã‚‹
    sub_df["EndDateStr"] = sub_df["Window"].str.split("_").str[-1]
    sub_df["Date"] = pd.to_datetime(sub_df["EndDateStr"], format='%Y%m%d')
    sub_df = sub_df.sort_values("Date")

    models = sub_df["Model_Type"].unique()
    pc_cols = [c for c in sub_df.columns if c.startswith("PC")]

    fig, axes = plt.subplots(len(models), 1, figsize=(12, 4 * len(models)), sharex=True)
    if len(models) == 1: axes = [axes]

    for ax, m_type in zip(axes, models):
        m_data = sub_df[sub_df["Model_Type"] == m_type]
        active_pcs = [c for c in pc_cols if not m_data[c].isna().all()]

        for col in active_pcs:
            # Xè»¸ã«æ–‡å­—åˆ—ã§ã¯ãªãã€ŒDateã€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¸¡ã™
            ax.plot(m_data["Date"], m_data[col], marker='o', label=col, lw=2)

        ax.set_title(f"Target: {target_var} | Model: {m_type}", fontsize=12, fontweight='bold')
        ax.axhline(0.3, color='red', ls='--', alpha=0.6, label="LB: 0.3")
        ax.axhline(0, color='black', lw=1, alpha=0.3)

        # --- ã€ã“ã“ãŒãƒã‚¤ãƒ³ãƒˆï¼šXè»¸ã®ç›®ç››ã‚Šã‚’ç¶ºéº—ã«ã™ã‚‹ã€‘ ---
        ax.xaxis.set_major_locator(mdates.YearLocator()) # 1å¹´åˆ»ã¿
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y/%m')) # è¡¨ç¤ºå½¢å¼

        ax.set_ylabel("Coefficient")
        ax.legend(loc='upper right', fontsize=9)
        ax.grid(axis='both', alpha=0.2)

    plt.xticks(rotation=45)
    plt.tight_layout()

    save_path = output_root / f"ä¿‚æ•°æ¨ç§»_{target_var}_æ—¥ä»˜ä¿®æ­£ç‰ˆ.png"
    plt.savefig(save_path, dpi=200, facecolor='white')
    plt.close()
    print(f"âœ… æ—¥ä»˜ã‚’ä¿®æ­£ã—ã¦ä¿å­˜ã—ã¾ã—ãŸ: {save_path}")

def splice_points_every_k(s: pd.Series, k: int = 4):
    """
    s: äºˆæ¸¬Seriesï¼ˆNaNã§ã‚®ãƒ£ãƒƒãƒ—ãŒå…¥ã‚‹æƒ³å®šï¼‰
    é€£ç¶šåŒºé–“ã”ã¨ã«ã€Œå…ˆé ­ + kç‚¹ã”ã¨ã€ã®indexã‚’è¿”ã™
    ä¾‹: k=4 ãªã‚‰ 0,4,8,... (=èµ¤ä¸¸â†’3ç‚¹â†’èµ¤ä¸¸â€¦)
    """
    idxs = []

    # éNaNã®ä½ç½®ã ã‘å–ã‚Šå‡ºã—
    valid = s.dropna()
    if valid.empty:
        return valid.index[:0]

    # å…ƒã®indexä¸Šã§ã€Œé€£ç¶šåŒºé–“ã€ã‚’æ¤œå‡ºï¼ˆNaNã§åˆ‡ã‚Œã¦ã„ã‚‹å‰æï¼‰
    is_valid = s.notna()
    # é€£ç¶šåŒºé–“IDï¼ˆNaNâ†’å€¤ã‚ã‚Šã§+1ï¼‰
    run_id = (is_valid & ~is_valid.shift(1, fill_value=False)).cumsum()
    run_id = run_id.where(is_valid)

    for rid, part in s[is_valid].groupby(run_id[is_valid]):
        part = part.sort_index()
        # 0, k, 2k, ... ç•ªç›®ã‚’æ¥ç€ç‚¹ã¨ã—ã¦æ¡ç”¨
        take = part.iloc[::k]
        idxs.extend(list(take.index))

    return pd.Index(idxs)

def plot_forecasts_all(actual, pred_by_h, out_png, title, shade=None):
    fig, ax = plt.subplots(figsize=(14, 5))

    # å®Ÿç¸¾ï¼šé»’
    ax.plot(actual.index, actual.values, linewidth=2.2, color="black", label="å®Ÿç¸¾å€¤", zorder=2)

    colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3']  # P1..P4

    for h in [1, 2, 3, 4]:
        s = pred_by_h[h]
        ax.plot(
            s.index, s.values,
            linestyle="--", marker="o", markersize=3,
            color=colors[h-1],
            label=f"äºˆæ¸¬ P{h}", zorder=4
        )

        # æ¥ç€ç‚¹ï¼ˆ4ç‚¹ã”ã¨ï¼‰â€»å‡¡ä¾‹ã«ã¯å‡ºã•ãªã„
        sp = splice_points_every_k(s, k=4)
        ax.scatter(
            sp, s.loc[sp],
            s=170, facecolors="none",
            edgecolors=colors[h-1], linewidths=2.6,
            zorder=6
        )

    # äºˆæ¸¬ç¯„å›²ï¼šã‚°ãƒ¬ãƒ¼ï¼ˆå‡¡ä¾‹ã¯1å›ã ã‘ï¼‰
    if shade is not None:
        ax.axvspan(shade[0], shade[1], color="lightgray", alpha=0.25, label="äºˆæ¸¬å¯¾è±¡æœŸé–“", zorder=1)

    ax.xaxis.set_major_locator(mdates.YearLocator(1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
    ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[4,7,10,1]))
    ax.grid(True, which='major', axis='x', linestyle='--', alpha=0.25)
    ax.grid(True, which='minor', axis='x', linestyle=':',  alpha=0.10)
    plt.setp(ax.get_xticklabels(), rotation=0, ha='center')

    ax.grid(True, which='major', axis='x', linestyle='--', alpha=0.25)
    ax.set_title(title)
    ax.set_xlabel("å¹´æœˆ")
    ax.set_ylabel("æ°´æº– (Level)")
    ax.grid(True, alpha=0.3)

    # â˜…å‡¡ä¾‹ã‚’ã•ã‚‰ã«ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã«ï¼ˆåˆ—æ•°ã‚’å¢—ã‚„ã—ã¦æ¨ªã«ä¼¸ã°ã™ï¼‰
    ax.legend(loc="upper left", ncol=3, frameon=True)

    fig.tight_layout()
    fig.savefig(out_png, dpi=200)
    plt.close(fig)

def plot_forecast_each(actual, pred_by_h, out_dir, base_title, shade=None):
    colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3']
    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    for h in [1, 2, 3, 4]:
        s = pred_by_h[h]

        fig, ax = plt.subplots(figsize=(14, 5))

        # å®Ÿç¸¾ã¯é»’
        ax.plot(actual.index, actual.values, linewidth=2.2, color="black", label="å®Ÿç¸¾å€¤", zorder=2)

        # äºˆæ¸¬ï¼ˆPã”ã¨ï¼‰
        ax.plot(
            s.index, s.values,
            color=colors[h-1], linestyle="--",
            marker="o", markersize=3,
            label=f"äºˆæ¸¬ P{h}", zorder=4
        )

        # é–‹å§‹ç‚¹ï¼ˆè‰²ã¯ç·šã¨åŒã˜ï¼‰
        sp = splice_points_every_k(s, k=4)   # â† 4ç‚¹ã”ã¨ï¼ˆ0,4,8,...ï¼‰/åŒºé–“ã”ã¨

        ax.scatter(
            sp, s.loc[sp],
            s=160, facecolors="none",
            edgecolors=colors[h-1], linewidths=2.4,
            label="æ¥ç€ç‚¹ï¼ˆ4ç‚¹ã”ã¨ï¼‰", zorder=6
        )

        # äºˆæ¸¬å¯¾è±¡æœŸé–“ã®èƒŒæ™¯ï¼ˆã‚°ãƒ¬ãƒ¼ï¼‰
        if shade is not None:
            ax.axvspan(shade[0], shade[1], color="lightgray", alpha=0.25, label="äºˆæ¸¬å¯¾è±¡æœŸé–“", zorder=1)

        ax.xaxis.set_major_locator(mdates.YearLocator(1))
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
        ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[4,7,10,1]))
        ax.grid(True, which='major', axis='x', linestyle='--', alpha=0.25)
        ax.grid(True, which='minor', axis='x', linestyle=':',  alpha=0.10)
        plt.setp(ax.get_xticklabels(), rotation=0, ha='center')

        ax.set_title(f"{base_title} (P{h})")
        ax.set_xlabel("å¹´æœˆ")
        ax.set_ylabel("æ°´æº– (Level)")
        ax.grid(True, alpha=0.3)
        ax.legend(loc="best")
        fig.tight_layout()
        fig.savefig(out_dir / f"forecast_P{h}.png", dpi=200)
        plt.close(fig)

def export_forecast_csv(actual, pred_by_h, out_csv):
    full_pred = make_full_pred(pred_by_h)
    df_out = actual.to_frame("Actual")
    df_out["Pred_Full"] = full_pred
    for h in [1,2,3,4]:
        df_out[f"Pred_P{h}"] = pred_by_h[h]
    df_out.to_csv(out_csv, encoding="utf-8-sig")

# =========================================================
# 3. ãƒ¡ã‚¤ãƒ³å‡¦ç†
# =========================================================
def main():
    # setup_japanese_font(r"C:\Windows\Fonts\YuGothR.ttc")
    apply_style_and_jpfont("default")
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†ï¼ˆã“ã“ã¯å…±é€šï¼‰
    df_raw = pd.read_csv(CONFIG["data_path"], parse_dates=["Date"])
    df_raw = df_raw.set_index("Date").sort_index()
    sector_df, macro_df, common_idx, meta = prepare_aligned(df_raw)

    # --- Moving Window è¨­å®š ---
    test_len = CONFIG["test_steps"]
    n_total = len(common_idx)
    all_coefficients_records=[]

    # ãƒ†ã‚¹ãƒˆæœŸé–“ã‚’1æœŸãšã¤ãšã‚‰ã—ã¦å…¨çµ„ã¿åˆã‚ã›ã‚’å®Ÿè¡Œ
    for i in range(n_total - test_len + 1):
        # ãƒ†ã‚¹ãƒˆæœŸé–“ã¨è¨“ç·´æœŸé–“ï¼ˆãƒ†ã‚¹ãƒˆä»¥å¤–ã™ã¹ã¦ï¼‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        te_idx_list = np.arange(i, i + test_len)
        tr_idx_list = np.setdiff1d(np.arange(n_total), te_idx_list)

        # ãƒ•ã‚©ãƒ«ãƒ€åç”¨ã®æ—¥ä»˜å–å¾—
        d_start = common_idx[te_idx_list[0]]
        d_end = common_idx[te_idx_list[-1]]

        # ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®šï¼ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã”ã¨ã«ä½œæˆï¼‰
        root_dir = Path(CONFIG["output_dir"]) / f"Window_Test_{d_start:%Y%m%d}_{d_end:%Y%m%d}"
        root_dir.mkdir(parents=True, exist_ok=True)

        print(f"â–¶ï¸ å®Ÿè¡Œä¸­: {root_dir.name}")

        # =========================================================
        # PCA: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§Fitã—ã€å…¨æœŸé–“ã‚’Transformï¼ˆPCã¯3å›ºå®šï¼‰
        # =========================================================
        n_pcs = CONFIG["pc_max"]  # å¸¸ã«3

        sc_pca = StandardScaler()
        X_tr_pca = sc_pca.fit_transform(sector_df.iloc[tr_idx_list])

        pca = PCA(n_components=n_pcs).fit(X_tr_pca)
        pc_cols_orig = [f"PC{k+1}" for k in range(n_pcs)]  # ["PC1","PC2","PC3"]

        # â˜…è¿½åŠ ï¼šã“ã“ã§ç¬¦å·ã‚’ç¢ºå®šï¼ˆinplaceï¼‰
        fix_pca_sign_inplace(
            pca=pca,
            feature_names=sector_df.columns,     # â† PCAã«ä½¿ã£ãŸåˆ—é †ã¨ä¸€è‡´ã—ã¦ã‚‹ã“ã¨ãŒé‡è¦
            pc_cols=pc_cols_orig,
            sector_groups=SECTOR_GROUPS,
            top_n=5,
        )

        # å…¨æœŸé–“ã®PCã‚¹ã‚³ã‚¢ï¼ˆPC1..PC3ï¼‰
        full_pcs = pca.transform(sc_pca.transform(sector_df))
        pc_all = pd.DataFrame(full_pcs, index=sector_df.index, columns=pc_cols_orig)

        # å·®åˆ†ï¼ˆPC1..PC3ï¼‰
        pc_diff_all = pc_all.diff()
        pc_cols_diff = [f"PC{k+1}_DIFF" for k in range(n_pcs)]  # ["PC1_DIFF","PC2_DIFF","PC3_DIFF"]
        pc_diff_all.columns = pc_cols_diff

        # --- å…ƒã®åˆ†æãƒ¬ãƒãƒ¼ãƒˆ(analyze_pca_details)å®Ÿè¡Œ ---
        # å¼•æ•°ã¯Windowã«åˆã‚ã›ã¦èª¿æ•´
        analyze_pca_details(
            pca, sc_pca, sector_df, pc_cols_orig, root_dir,
            tr_idx_list=tr_idx_list, te_idx_list=te_idx_list
        )
        # ãƒã‚¯ãƒ­åŒæœŸ
        m_df_win = macro_df.loc[pc_diff_all.index]
        combinations = [[]] + [list(c) for r in range(1, n_pcs+1) for c in itertools.combinations(pc_cols_diff, r)]
        summary_list = []

        # combinations ä½œã£ãŸç›´å¾Œï¼ˆfor combo ã®å‰ï¼‰ã«ç§»å‹•
        print("macro cols:", list(m_df_win.columns))
        print("combo sample:", combinations[:5], " ... total:", len(combinations))

        # 1. ã™ã¹ã¦ã®åˆ¶ç´„ã‚’ã“ã“ã«é›†ç´„ (ç¯„å›²æŒ‡å®š or ç¬¦å·æŒ‡å®š)
        STRICT_CONSTRAINTS = {
            ("PC1", "GDP"): (0.2, np.inf),
            ("PC1", "NIKKEI"): (0.2, np.inf),
            ("PC1", "UNEMP_RATE"): (-np.inf, -0.1),
            ("PC2", "GDP"): (0.05, np.inf),
            ("PC2", "NIKKEI"): (0.05, np.inf),
            ("PC2", "UNEMP_RATE"): (-np.inf, -0.05),
            # ä»¥å‰ã® BASE_SIGN_CONSTRAINTS ç›¸å½“ã‚‚ã“ã“ã«æ›¸ã„ã¦ãŠã‘ã°ã‚¨ãƒ©ãƒ¼ã«ãªã‚Šã¾ã›ã‚“
            # ("*", "UNEMP_RATE"): (-np.inf, -0.3), # å¿…è¦ãªã‚‰è¿½åŠ 
        }

        LAG_CONSTRAINTS = {
            # (target, pred): (lb, ub)
            # ä¾‹ï¼‰å¤±æ¥­ç‡æ–¹ç¨‹å¼ã§ã€GDPãƒ©ã‚°ã¯è² 
            # ("UNEMP_RATE", "GDP"): (100, np.inf),

            # ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰ã‚‚OK
            # ("*", "JGB_10Y"): (0.0, 0.0),   # ä¾‹ï¼šJGB_10Yã®ãƒ©ã‚°åŠ¹æœã‚’å…¨éƒ¨ã‚¼ãƒ­
        }

        for combo in combinations:
            m_name_display = ", ".join(combo) if combo else "ãƒã‚¯ãƒ­ã®ã¿"
            sub_dir = root_dir / ("_".join(combo) if combo else "BASE_VAR_ONLY")
            sub_dir.mkdir(parents=True, exist_ok=True)

            # --- è¨“ç·´ã¨ãƒ†ã‚¹ãƒˆã®åˆ‡ã‚Šå‡ºã— ---
            win_idx = m_df_win.index
            te_dates = win_idx[te_idx_list]   # â† ã“ã‚Œã§å¿…ãš index å†…
            tr_pre_dates = common_idx[tr_idx_list[tr_idx_list < te_idx_list[0]]]
            tr_post_dates = common_idx[tr_idx_list[tr_idx_list > te_idx_list[-1]]]

            # 1. éå»å´ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿
            tr_pre = pd.concat([m_df_win.loc[win_idx.isin(tr_pre_dates)],
                                pc_diff_all.loc[win_idx.isin(tr_pre_dates), combo]], axis=1)

            # 2. æœªæ¥å´ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿
            tr_post = pd.concat([m_df_win.loc[win_idx.isin(tr_post_dates)],
                                 pc_diff_all.loc[win_idx.isin(tr_post_dates), combo]], axis=1)

            te_raw = pd.concat([
                m_df_win.loc[win_idx.isin(te_dates)],
                pc_diff_all.loc[win_idx.isin(te_dates), combo]
            ], axis=1)
            # ã€é‡è¦ã€‘æœªæ¥å´ãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­1è¡Œã‚’NaNã«ã™ã‚‹ï¼ˆãƒ©ã‚°è¨ˆç®—ã§éå»å´ã¨ç¹‹ãŒã‚‹ã®ã‚’é˜²ããŸã‚ï¼‰
            tr_raw_stats = pd.concat([tr_pre, tr_post])  # å£ãªã—ï¼ˆçµ±è¨ˆã¯è‡ªç„¶ã«ï¼‰
            tr_post_design = tr_post.copy()
            if not tr_post_design.empty:
                tr_post_design.iloc[0, :] = np.nan       # å£ã‚ã‚Šï¼ˆè¨­è¨ˆè¡Œåˆ—ã®ã¿ï¼‰
            tr_raw_design = pd.concat([tr_pre, tr_post_design])

            means = tr_raw_stats.mean()
            stds  = tr_raw_stats.std(ddof=0).replace(0, 1.0)

            tr_s = (tr_raw_design - means) / stds
            te_s = (te_raw        - means) / stds
            m_cols = list(m_df_win.columns)
            m_dim = len(m_cols)

            gdp_col = next(k for k in m_cols if meta[k]["orig"] == "GDP")
            gdp_j = m_cols.index(gdp_col)

            # ãƒ¢ãƒ‡ãƒ«æ¨å®š
            Y_tr, X_tr, idx_tr = make_design(tr_s[m_cols], tr_s[combo] if combo else None, CONFIG["p_lag"])

            if CONFIG.get("use_constraints", True):
                beta_list = []
                ridge_aug = np.sqrt(CONFIG["ridge"]) * np.eye(X_tr.shape[1])
                X_aug = np.vstack([X_tr, ridge_aug])

                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åãƒªã‚¹ãƒˆ
                param_names = ["CONST"] + [f"LAG_{m}" for m in m_cols] + list(combo)

                for j, target_m_col in enumerate(m_cols):
                    orig_target = meta[target_m_col]["orig"]
                    Y_aug = np.concatenate([Y_tr[:, j], np.zeros(X_tr.shape[1])])
                    lb = np.full(X_aug.shape[1], -np.inf)
                    ub = np.full(X_aug.shape[1], np.inf)

                    # --- ä¿®æ­£ç‰ˆï¼šãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰å¯¾å¿œã®åˆ¶ç´„é©ç”¨ãƒ­ã‚¸ãƒƒã‚¯ ---
                    if combo:
                        start_exog_idx = 1 + m_dim
                        for k, exog_pc_name in enumerate(combo):
                            orig_exog = exog_pc_name.split("_")[0]
                            target_idx = start_exog_idx + k

                            # å„ªå…ˆé †ä½ã‚’ã¤ã‘ã¦åˆ¶ç´„ã‚’æ¢ã™
                            # 1. å€‹åˆ¥æŒ‡å®š ("PC1", "GDP")
                            # 2. å¤–ç”Ÿå¤‰æ•°å…¨æŒ‡å®š ("*", "GDP")
                            # 3. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå…¨æŒ‡å®š ("PC1", "*")
                            # 4. å…¨æŒ‡å®š ("*", "*")
                            bound = (STRICT_CONSTRAINTS.get((orig_exog, orig_target)) or
                                    STRICT_CONSTRAINTS.get(("*", orig_target)) or
                                    STRICT_CONSTRAINTS.get((orig_exog, "*")) or
                                    STRICT_CONSTRAINTS.get(("*", "*")))

                            if bound:
                                lb[target_idx] = bound[0]
                                ub[target_idx] = bound[1]
                            # --- ã‚¨ãƒ©ãƒ¼ã®å…ƒã ã£ãŸ else ç¯€ (old_c) ã¯å‰Šé™¤ã—ã¾ã—ãŸ ---

                    # =============================
                    # A(ãƒ©ã‚°ä¿‚æ•°)ã®åˆ¶ç´„ï¼ˆA1ã®è¦ç´ åˆ¶ç´„ï¼‰
                    # =============================
                    # lagä¿‚æ•°ãƒ–ãƒ­ãƒƒã‚¯ã¯ param_names ã® ["CONST"] ã®æ¬¡ãªã®ã§ index=1 ã‹ã‚‰å§‹ã¾ã‚‹
                    for pred_i, pred_m_col in enumerate(m_cols):
                        pred_orig = meta[pred_m_col]["orig"]     # ä¾‹: "GDP"
                        target_idx = 1 + pred_i                  # CONSTã®æ¬¡ãŒLAGç¾¤

                        bound = (LAG_CONSTRAINTS.get((orig_target, pred_orig)) or
                                LAG_CONSTRAINTS.get(("*", pred_orig)) or
                                LAG_CONSTRAINTS.get((orig_target, "*")) or
                                LAG_CONSTRAINTS.get(("*", "*")))

                        if bound:
                            lb[target_idx] = bound[0]
                            ub[target_idx] = bound[1]
                            print("APPLY LAG CONSTRAINT",
                            "target=", orig_target,
                            "pred=", pred_orig,
                            "idx=", target_idx,
                            "lb,ub=", bound)

                    res = lsq_linear(X_aug, Y_aug, bounds=(lb, ub), lsmr_tol='auto')
                    if not res.success:
                        # ä¿é™ºï¼šåˆ¶ç´„ã‚’ç„¡è¦–ã—ã¦é€šå¸¸ã®ridgeï¼ˆã¾ãŸã¯OLSï¼‰ã«è½ã¨ã™
                        res = lsq_linear(X_aug, Y_aug, bounds=(-np.inf, np.inf), lsmr_tol='auto')
                    beta_list.append(res.x)

                    record = {
                        "Window": root_dir.name,
                        "Model_Type": m_name_display,
                        "Target_Variable": orig_target,
                    }
                    # å„ä¿‚æ•°ã‚’ã‚«ãƒ©ãƒ ã¨ã—ã¦è¿½åŠ 
                    for name, val in zip(param_names, res.x):
                        record[name] = val

                    # mainã®å†’é ­ã§å®šç¾©ã—ãŸ all_coefficients_records ã«è¿½åŠ 
                    all_coefficients_records.append(record)

                Beta = np.array(beta_list).T
            else:
                # ä¿é™ºï¼šåˆ¶ç´„ã‚’åˆ‡ã£ãŸå ´åˆã§ã‚‚è½ã¡ãªã„ã‚ˆã†ã«OLS
                Beta = np.linalg.lstsq(X_tr, Y_tr, rcond=None)[0]

            # --- æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«æ¨å®šç›´å¾Œã‹ã‚‰ ---
            # ===== trainã® fittedï¼ˆæ¨™æº–åŒ–ç©ºé–“â†’å…ƒã‚¹ã‚±ãƒ¼ãƒ«ã¸ï¼‰=====
            fitted_scaled = X_tr @ Beta
            means_vec = means[m_cols].values
            stds_vec  = stds[m_cols].values

            fitted_change = fitted_scaled * stds_vec + means_vec
            fitted_change_df = pd.DataFrame(fitted_change, index=idx_tr, columns=m_cols)

            # ===== ãƒ¬ãƒ™ãƒ«ã«å¾©å…ƒï¼ˆå½“ã¦ã¯ã‚å€¤ï¼‰=====
            # 1) idx_tr ä¸Šã§ fitted_level_df ã‚’ä½œã‚‹
            fitted_level_df = pd.DataFrame(index=idx_tr)

            for m_col in m_cols:
                orig_name = meta[m_col]["orig"]
                meth      = meta[m_col]["method"]

                if meth == "LEVEL":
                    fitted_level = fitted_change_df[m_col]
                else:
                    prev_actual = df_raw[orig_name].shift(1).reindex(idx_tr)
                    chg = fitted_change_df[m_col]
                    if meth == "LOGDIFF":
                        fitted_level = prev_actual * np.exp(chg)
                    elif meth == "DIFF":
                        fitted_level = prev_actual + chg
                    elif meth == "PCTCHANGE":
                        fitted_level = prev_actual * (1.0 + chg)
                    else:
                        fitted_level = prev_actual + chg

                fitted_level_df[f"{orig_name}_Fitted"] = fitted_level

            # 2) å…¨æœŸé–“ index ã«åŸ‹ã‚æˆ»ã—
            fitted_level_df_full = pd.DataFrame(index=m_df_win.index)
            for m_col in m_cols:
                orig_name = meta[m_col]["orig"]
                col = f"{orig_name}_Fitted"
                fitted_level_df_full[col] = np.nan
                fitted_level_df_full.loc[idx_tr, col] = fitted_level_df[col].values

            # 3) ãƒ†ã‚¹ãƒˆæœŸé–“ã¯ NaN ã§åˆ‡æ–­
            fitted_level_df_full.loc[fitted_level_df_full.index.intersection(te_dates), :] = np.nan

            # æŒ‡æ¨™è¨ˆç®—
            tr_resid = Y_tr - X_tr @ Beta
            n_obs = len(Y_tr)
            tr_rmse = float(np.sqrt(np.nanmean(tr_resid**2)))

            # ===== è¿½åŠ ï¼šå¤‰æ•°åˆ¥RMSEï¼ˆtrainï¼‰=====
            tr_rmse_by = np.sqrt(np.nanmean(tr_resid**2, axis=0))  # (m_dim,)

            # ä¾‹ï¼šGDPã®train RMSEã ã‘æŠœ
            tr_rmse_gdp = float(tr_rmse_by[gdp_j])

            # --- ãƒ†ã‚¹ãƒˆã®ç›´å‰1æœŸã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦è¿½åŠ ï¼ˆp_lag=1æƒ³å®šï¼‰ ---
            p = CONFIG["p_lag"]

            # ãƒ†ã‚¹ãƒˆé–‹å§‹æ—¥ã®ç›´å‰æ—¥ï¼ˆcommon_idxä¸Šï¼‰
            te_start_date = te_dates[0]
            pos = np.where(common_idx == te_start_date)[0][0]
            if pos - p < 0:
                # æœ€åˆã™ãã‚‹å ´åˆã¯äºˆæ¸¬ä¸å¯
                Y_te = X_te = idx_te = None
            else:
                ctx_dates = common_idx[pos-p:pos]  # ç›´å‰pæœŸ
                # æ¨™æº–åŒ–æ¸ˆã¿ series ã‚’ä½œã‚‹ï¼ˆm_cols ã¨ combo ã‚’å«ã‚€å½¢ï¼‰
                ctx_raw = pd.concat([
                    m_df_win.loc[ctx_dates, m_cols],
                    pc_diff_all.loc[ctx_dates, combo] if combo else pd.DataFrame(index=ctx_dates)
                ], axis=1)

                # ctx ã‚‚ train ã® means/stds ã§æ¨™æº–åŒ–ï¼ˆã“ã“é‡è¦ï¼‰
                ctx_s = (ctx_raw - means) / stds

                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ + ãƒ†ã‚¹ãƒˆ ã‚’ç¸¦çµåˆã—ã¦ design ä½œã‚‹
                need_cols = m_cols + list(combo)
                ctx_s = ctx_s.reindex(columns=need_cols)
                te_s  = te_s.reindex(columns=need_cols)
                te_raw2 = pd.concat([ctx_s, te_s], axis=0)

                Y_te, X_te, idx_te = make_design(te_raw2[m_cols], te_raw2[combo] if combo else None, p)

                # ã“ã“ã§ idx_te ã¯ã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚“ã å¾Œã® valid è¡Œã€ãªã®ã§ã€
                # äºˆæ¸¬å¯¾è±¡ã‚’ãƒ†ã‚¹ãƒˆéƒ¨åˆ†ã«çµã‚‹
                is_test = idx_te.isin(te_dates)
                Y_te = Y_te[is_test]
                X_te = X_te[is_test]
                idx_te = idx_te[is_test]
            te_rmse_gdp = np.nan
            if Y_te is None or X_te is None or idx_te is None or len(Y_te) == 0:
                te_rmse = np.nan
            else:
                te_resid = Y_te - X_te @ Beta
                te_rmse = float(np.sqrt(np.nanmean(te_resid**2)))
                # ===== è¿½åŠ ï¼šå¤‰æ•°åˆ¥RMSEï¼ˆtestï¼‰=====
                te_rmse_by = np.sqrt(np.nanmean(te_resid**2, axis=0))  # (m_dim,)

                # ä¾‹ï¼šGDPã®test RMSEã ã‘æŠœ
                te_rmse_gdp = float(te_rmse_by[gdp_j])

            # --- AICè¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ã®å¼·åŒ–ç‰ˆ ---
            sigma_matrix = (tr_resid.T @ tr_resid) / n_obs
            sign, logdet = np.linalg.slogdet(sigma_matrix)

            if sign > 0:
                # é€šå¸¸ã®è¨ˆç®—
                aic_val = n_obs * logdet + 2 * Beta.size
            else:
                # è¡Œåˆ—å¼ãŒæ­£å¸¸ã«è¨ˆç®—ã§ããªã„å ´åˆã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
                # æ­£ã®å›ºæœ‰å€¤ã®ã¿ã‚’æŠ½å‡ºã—ã¦å¯¾æ•°å’Œã‚’ã¨ã‚‹
                eigs = np.linalg.eigvalsh(sigma_matrix)
                valid_eigs = eigs[eigs > 1e-15] # ã»ã¼ã‚¼ãƒ­ä»¥ä¸Šã®å›ºæœ‰å€¤ã®ã¿
                if len(valid_eigs) > 0:
                    aic_val = n_obs * np.sum(np.log(valid_eigs)) + 2 * Beta.size
                else:
                    aic_val = np.nan
            # -------------------------------

            A1 = Beta[1:1+m_dim, :].T
            max_eig_abs = np.max(np.abs(np.linalg.eigvals(A1)))

            summary_list.append({
                "ãƒ¢ãƒ‡ãƒ«æ§‹æˆ": m_name_display,
                "è¨“ç·´RMSE": round(tr_rmse, 4),
                "è¨“ç·´RMSE_GDP": round(tr_rmse_gdp, 4),
                "äºˆæ¸¬RMSE": round(te_rmse, 4) if not np.isnan(te_rmse) else np.nan,
                "äºˆæ¸¬RMSE_GDP": round(te_rmse_gdp, 4),
                "RMSEæ¯”": round(te_rmse / tr_rmse, 2) if tr_rmse > 0 and not np.isnan(te_rmse) else np.nan,
                "AIC": round(aic_val, 2) if not np.isnan(aic_val) else np.nan,
                "æœ€å¤§å›ºæœ‰å€¤": round(max_eig_abs, 3)
            })

            # --- äºˆæ¸¬çµæœã®å¯è¦–åŒ– (é€æ¬¡äºˆæ¸¬ & å…¨ä½“è¡¨ç¤ºç‰ˆ) ---
            # 1. é€æ¬¡äºˆæ¸¬ (Dynamic Forecast) ã®å®Ÿè¡Œ
            y_te_pred_list = []
            if X_te is not None and len(X_te) > 0:
                curr_X = X_te[0:1, :]

                for t in range(len(idx_te)):  # â† test_steps ã§ã¯ãªã idx_te ã«åˆã‚ã›ã‚‹
                    pred_t = curr_X @ Beta
                    y_te_pred_list.append(pred_t)

                    if t < len(idx_te) - 1:
                        next_lag_y = pred_t

                        if combo:
                            # æ¬¡ã®æ™‚ç‚¹ã® exog ã‚’ idx_te[t+1] ã§å–ã‚‹ï¼ˆå®‰å…¨ï¼‰
                            d_next = idx_te[t+1]
                            next_exog = te_s.loc[[d_next], combo].values
                        else:
                            next_exog = np.empty((1, 0))

                        curr_X = np.concatenate([[[1.0]], next_lag_y, next_exog], axis=1)

                y_te_pred_scaled = np.vstack(y_te_pred_list)
            else:
                y_te_pred_scaled = None

            # i=0 (æœ€åˆæœŸ) ã‚’é™¤å¤–ã—ã€ã‹ã¤äºˆæ¸¬å€¤ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å®Ÿè¡Œ
            if y_te_pred_scaled is not None and i != 0:

                # 1. äºˆæ¸¬æœŸé–“ã®æ—¥ä»˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
                start_d_idx = np.where(common_idx == te_dates[0])[0][0]
                te_actual_dates = idx_te

                # å…¨æœŸé–“è¡¨ç¤ºï¼ˆ2008-
                # plot_start_idx = max(0, start_d_idx - 8)
                # plot_end_idx = min(len(common_idx), start_d_idx + CONFIG["test_steps"] + 4)
                # full_display_range = common_idx[plot_start_idx:plot_end_idx]    
                full_display_range = df_raw.index  # å®Ÿç¸¾ã¯ã“ã‚Œã§å…¨æœŸé–“

                for i_m, m_col in enumerate(m_cols):
                    orig_name = meta[m_col]["orig"]
                    meth = meta[m_col]["method"]

                    # 3. æ°´æº–å¾©å…ƒãƒ­ã‚¸ãƒƒã‚¯
                    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°è§£é™¤
                    pred_change = (y_te_pred_scaled[:, i_m] * stds[m_col]) + means[m_col]
                    # i != 0 ãªã®ã§å¿…ãšç›´å‰ã®å®Ÿç¸¾(ãƒ©ã‚°)
                    hist_before = df_raw.loc[df_raw.index < te_actual_dates[0], orig_name].dropna()
                    if hist_before.empty:
                        continue
                    last_actual_level = hist_before.iloc[-1]

                    pred_levels = []
                    curr_level = last_actual_level
                    for val in pred_change:
                        if meth == "LOGDIFF":
                            curr_level = curr_level * np.exp(val)
                        elif meth == "DIFF":
                            curr_level = curr_level + val
                        elif meth == "PCTCHANGE":
                            curr_level = curr_level * (1.0 + val)
                        elif meth == "LEVEL":
                            curr_level = val
                        else:
                            curr_level = curr_level + val
                        pred_levels.append(curr_level)

                    # --- äºˆæ¸¬å€¤ã®è¨ˆç®—ãŒçµ‚ã‚ã£ãŸç›´å¾Œã«é…ç½® ---
                    p_df = pd.DataFrame(pred_levels, index=te_actual_dates, columns=[f"{orig_name}_Pred"])
                    # å¤‰æ•°åä»˜ãã§ä¿å­˜ã™ã‚‹ã“ã¨ã§ã€å¾Œã®é›†è¨ˆã‚’ç¢ºå®Ÿã«ã—ã¾ã™
                    p_df.to_csv(sub_dir / f"äºˆæ¸¬å€¤_æ°´æº–ãƒ™ãƒ¼ã‚¹_{orig_name}.csv", encoding="utf-8-sig")

                    # --- 4. ã‚°ãƒ©ãƒ•æç”» ---
                    plt.figure(figsize=(11, 5.5))

                    # å®Ÿç¸¾å€¤ (é»’) ã¨ äºˆæ¸¬å€¤ (èµ¤)
                    # å®Ÿç¸¾ï¼šè–„ã‚ã€å°ã•ã‚
                    plt.plot(
                        full_display_range, df_raw.loc[full_display_range, orig_name],
                        label="å®Ÿç¸¾å€¤",
                        color="#333333", lw=1.5,
                        marker='o', markersize=3, alpha=0.7,
                        zorder=2
                    )
                    # äºˆæ¸¬ï¼šå¤ªã‚ã€ç‚¹å¤§ãã‚ã€ç™½ç¸ã§åŸ‹ã‚‚ã‚Œé˜²æ­¢ã€å‰é¢
                    plt.plot(
                        te_actual_dates, pred_levels,
                        label="äºˆæ¸¬å€¤ (é€æ¬¡)",
                        color="red", lw=1.5, linestyle="--",
                        marker="o", markersize=3,
                        markerfacecolor="red", markeredgecolor="red",
                        alpha=0.9, zorder=5
                    )
                    # è¿½åŠ ï¼šå½“ã¦ã¯ã‚ï¼ˆtrainã®1æœŸå…ˆå½“ã¦ã¯ã‚ï¼‰
                    plt.plot(
                        fitted_level_df_full.index,
                        fitted_level_df_full[f"{orig_name}_Fitted"],
                        label="å½“ã¦ã¯ã‚å€¤ (1æœŸå…ˆ)",
                        color="#1f77b4", lw=1.5, linestyle="--",
                        marker="o", markersize=3,
                        alpha=0.8, zorder=3
                    )
                    # ãƒ†ã‚¹ãƒˆæœŸé–“ã®èƒŒæ™¯
                    plt.axvspan(te_actual_dates[0], te_actual_dates[-1], color='gray', alpha=0.1, label='äºˆæ¸¬å¯¾è±¡æœŸé–“')

                    ax = plt.gca()
                    # --- xè»¸ï¼ˆé•·æœŸå‘ã‘ï¼‰ï¼šå¹´ã ã‘ãƒ©ãƒ™ãƒ«ã€å››åŠæœŸã¯è£œåŠ© ---
                    ax.xaxis.set_major_locator(mdates.YearLocator(base=1))          # 1å¹´åˆ»ã¿
                    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))        # "2008" ã¿ãŸã„ã«å¹´ã ã‘

                    ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[3,6,9,12]))  # å››åŠæœŸæœ«ã¯è£œåŠ©ç›®ç››ã‚Š
                    ax.grid(True, which='major', axis='x', linestyle='--', alpha=0.25)
                    ax.grid(True, which='minor', axis='x', linestyle=':',  alpha=0.10)
                    ax.grid(True, which='major', axis='y', linestyle='--', alpha=0.25)

                    plt.xticks(rotation=0)  # å¹´ã ã‘ãªã‚‰å›è»¢ãªã—ã§OK
                    plt.title(f"ã€äºˆæ¸¬ã€‘{orig_name} : {m_name_display}", fontsize=14)
                    plt.xlabel("å¹´æœˆ", fontsize=12)
                    plt.ylabel("æ°´æº– (Level)", fontsize=12)
                    plt.legend(loc='best', frameon=True, shadow=True)
                    plt.tight_layout()

                    # ä¿å­˜
                    plt.savefig(sub_dir / f"PRED_{orig_name}.png", dpi=150)
                    plt.close()

            # --- å…ƒã®IRFãƒ—ãƒ­ãƒƒãƒˆ(å¤‰æ›´ãªã—) ---
            if CONFIG["do_irf"] and combo:
                B = Beta[1+m_dim:, :].T
                for j, pc_label in enumerate(combo):
                    pc_folder = sub_dir / f"Shock_{pc_label}"; pc_folder.mkdir(parents=True, exist_ok=True)
                    impact = np.zeros((13, m_dim)); impact[1] = B[:, j]
                    for h in range(2, 13): impact[h] = A1 @ impact[h-1]
                    for i_m, m_col in enumerate(m_cols):
                        orig_m, meth_m = meta[m_col]["orig"], meta[m_col]["method"]
                        base = BASE_LEVELS.get(orig_m, 100); imp_raw = impact[:, i_m] * stds[m_col]
                        vals = [base]
                        for s in range(1, 13):
                            if meth_m == "LOGDIFF":
                                vals.append(vals[-1] * np.exp(imp_raw[s]))
                            elif meth_m == "DIFF":
                                vals.append(vals[-1] + imp_raw[s])
                            elif meth_m == "PCTCHANGE":
                                vals.append(vals[-1] * (1.0 + imp_raw[s]))
                            elif meth_m == "LEVEL":
                                vals.append(base + imp_raw[s])  # LEVELã¯â€œåŸºæº–å€¤ï¼‹æ°´æº–ã‚·ãƒ§ãƒƒã‚¯â€
                            else:
                                vals.append(vals[-1] + imp_raw[s])
                        plt.figure(figsize=(6, 3.5))
                        ax = plt.gca()
                        ax.yaxis.get_major_formatter().set_useOffset(False)
                        ax.yaxis.get_major_formatter().set_scientific(False)
                        plt.plot(range(13), vals, color='k', lw=1)
                        plt.scatter(range(13), vals, marker='o', s=15, color='k', zorder=2)
                        plt.scatter(1, vals[1], facecolors='none', edgecolors='k', marker='o', s=100, lw=1, zorder=3)
                        plt.axvline(x=1, color='k', ls='--', lw=0.7, alpha=0.6)
                        plt.axhline(base, color='gray', ls=':', lw=0.8)
                        plt.title(f"{pc_label} â†’ {orig_m}")
                        plt.grid(alpha=0.15); plt.tight_layout()
                        plt.savefig(pc_folder / f"{orig_m}_å¿œç­”.png"); plt.close()

        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã”ã¨ã®ã‚µãƒãƒªãƒ¼ä¿å­˜
        pd.DataFrame(summary_list).sort_values("AIC").to_csv(root_dir / "ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚µãƒãƒªãƒ¼.csv", index=False, encoding="utf-8-sig")

    # --- å®Ÿè¡Œéƒ¨åˆ† ---
    agg_root = Path(CONFIG["output_dir"]) / "00_aggregate"
    for d in ["model_eval", "pca", "heatmaps", "forecasts", "coefficients"]:
        (agg_root / d).mkdir(parents=True, exist_ok=True)

    aggregate_results(Path(CONFIG["output_dir"]), agg_root=agg_root)

    csv_file = agg_root / "model_eval" / "å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é›†è¨ˆ_ãƒ¢ãƒ‡ãƒ«è©•ä¾¡_è©³ç´°ç‰ˆ.csv"
    if csv_file.exists():
        visualize_model_performance(csv_file)

    plot_connected_forecasts(CONFIG["output_dir"], agg_root / "forecasts", df_raw, meta, "PC1_DIFF")
    plot_connected_forecasts(CONFIG["output_dir"], agg_root / "forecasts", df_raw, meta, "BASE_VAR_ONLY")

    if all_coefficients_records:
        df_coef = pd.DataFrame(all_coefficients_records)
        coef_path = agg_root / "coefficients" / "all_model_coefficients.csv"
        df_coef.to_csv(coef_path, index=False, encoding="utf-8-sig")

        for t in ["GDP", "NIKKEI", "USD_JPY"]:
            plot_exog_trends(coef_path, agg_root / "coefficients", target_var=t)

    print(f"âœ… å…¨å·¥ç¨‹å®Œäº†ã€‚{agg_root} ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

if __name__ == "__main__":
    main()
