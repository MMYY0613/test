import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from pathlib import Path
import itertools
import warnings
import seaborn as sns
from scipy.optimize import lsq_linear  # è¿½åŠ 

warnings.simplefilter('ignore')

# =========================================================
# 1. è¨­å®šãƒ»å®šæ•°
# =========================================================
CONFIG = {
    "data_path": "./data/all_q_merged_tmp2.csv",
    "output_dir": "./output_pca_final_tmp2",
    "train_range": ("2015-01-01", "2016-06-30"),
    "test_steps": 4,
    "pc_max": 3,
    "p_lag": 1,
    "ridge": 1.0,
    "do_irf": True,
}

BASE_LEVELS = {
    "GDP": 500000, "NIKKEI": 20000, "USD_JPY": 110, "UNEMP_RATE": 3.0,
    "JGB_1Y": 0.1, "JGB_2Y": 0.2, "JGB_3Y": 0.3, "CPI": 100
}

TARGET_MACRO = list(BASE_LEVELS.keys())

SECTOR_COLS = [
    "RET_FOODS", "RET_ENERGY_RESOURCES", "RET_CONSTRUCTION_MATERIALS", "RET_RAW_MAT_CHEM",
    "RET_PHARMACEUTICAL", "RET_AUTOMOBILES_TRANSP_EQUIP", "RET_STEEL_NONFERROUS",
    "RET_MACHINERY", "RET_ELEC_APPLIANCES_PRECISION", "RET_IT_SERV_OTHERS",
    "RET_ELECTRIC_POWER_GAS", "RET_TRANSPORT_LOGISTICS", "RET_COMMERCIAL_WHOLESALE",
    "RET_RETAIL_TRADE", "RET_BANKS", "RET_FIN_EX_BANKS", "RET_REAL_ESTATE", "RET_TEST"
]

mpl.rcParams["axes.unicode_minus"] = False
plt.rcParams['font.family'] = ["Hiragino Sans"]

# =========================================================
# 2. ãƒ­ã‚¸ãƒƒã‚¯é–¢æ•°
# =========================================================
def smart_transform(series, name):
    if (series <= 0).any() or "JGB" in name or "UNEMP" in name:
        return series.diff(), "DIFF"
    return np.log(series).diff(), "LOGDIFF"

def prepare_aligned(df_raw):
    m_work = pd.DataFrame(index=df_raw.index)
    meta = {}
    for col in TARGET_MACRO:
        if col in df_raw.columns:
            ts, method = smart_transform(df_raw[col], col)
            m_work[f"{col}_{method}"] = ts
            meta[f"{col}_{method}"] = {"orig": col, "method": method}
    m_df = m_work.dropna()
    s_raw = df_raw[[c for c in SECTOR_COLS if c in df_raw.columns]].interpolate(limit_direction='both')
    s_diff = s_raw.dropna() if all(c.startswith("RET_") for c in s_raw.columns) else s_raw.diff().dropna()
    common = s_diff.index.intersection(m_df.index)
    return s_diff.loc[common], m_df.loc[common], common, meta

def pca_no_leak(X_all, t_start, t_end, t_size, k_use=3):
    X_tr = X_all.iloc[t_start:t_end]
    X_te = X_all.iloc[t_end:t_end+t_size]
    sc = StandardScaler(); pca = PCA(n_components=k_use)
    Ztr = pca.fit_transform(sc.fit_transform(X_tr))
    Zte = pca.transform(sc.transform(X_te))
    cols = [f"PC{i+1}" for i in range(k_use)]
    pc_df = pd.concat([pd.DataFrame(Ztr, index=X_tr.index, columns=cols),
                       pd.DataFrame(Zte, index=X_te.index, columns=cols)])
    return pc_df.diff().dropna(), pca.explained_variance_ratio_

def make_design(endog, exog, p):
    Y = endog.values
    X_parts = [np.ones((len(endog), 1))]
    for lag in range(1, p+1):
        X_parts.append(endog.shift(lag).values)
    if exog is not None and not exog.empty:
        X_parts.append(exog.values)
    X = np.concatenate(X_parts, axis=1)
    valid = ~np.isnan(X).any(axis=1) & ~np.isnan(Y).any(axis=1)
    return Y[valid], X[valid]

def analyze_pca_details(pca, sector_df, pc_cols, root_dir, t_start, t_end, t_size):
    """PCAåˆ†æï¼šæ¨™æº–åŒ–ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®ç”Ÿã‚¹ã‚³ã‚¢ç®—å‡ºãƒ»ãƒ—ãƒ­ãƒƒãƒˆãƒ»è² è·é‡CSVå‡ºåŠ›ï¼ˆè»¸ãƒ©ãƒ™ãƒ«è¿½åŠ ç‰ˆï¼‰"""
    pca_dir = root_dir / "pca_analysis"
    pca_dir.mkdir(exist_ok=True)
    
    # è¨“ç·´+ãƒ†ã‚¹ãƒˆæœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¦æ¨™æº–åŒ–
    X_target = sector_df.iloc[t_start : t_end + t_size]
    sc = StandardScaler()
    sc.fit(sector_df.iloc[t_start : t_end])
    X_scaled = sc.transform(X_target)
    
    # ä¸»æˆåˆ†ã‚¹ã‚³ã‚¢ã®ç®—å‡º
    scores = pca.transform(X_scaled)
    score_df = pd.DataFrame(scores, index=X_target.index, columns=pc_cols)
    
    # ã‚¹ã‚³ã‚¢ã®CSVå‡ºåŠ›
    score_df.to_csv(pca_dir / "ä¸»æˆåˆ†ã‚¹ã‚³ã‚¢_ç”Ÿãƒ‡ãƒ¼ã‚¿.csv", encoding="utf-8-sig")

    expl = pca.explained_variance_ratio_
    components = pca.components_.copy()

    # ã‚»ã‚¯ã‚¿ãƒ¼ç¾¤ã®å®šç¾© (ä¾‹: ã‚ãªãŸãŒæ³¨ç›®ã—ãŸã„A, B, C)
    sector_groups = {
        "PC1": SECTOR_COLS, # PC1ã¯å…¨ã‚»ã‚¯ã‚¿ãƒ¼å¹³å‡ã§æ­£ï¼ˆæ™¯æ°—å…¨ä½“ï¼‰
        "PC2": ["RET_FOODS", "RET_RETAIL_TRADE", "RET_PHARMACEUTICAL"], # å†…éœ€ç³»
        "PC3": ["RET_MACHINERY", "RET_ELEC_APPLIANCES_PRECISION", "RET_STEEL_NONFERROUS"] # è¼¸å‡ºç³»
    }

    # è¨­å®šï¼šä¸Šä½ä½•ä»¶ã®ã‚»ã‚¯ã‚¿ãƒ¼ã§ç¬¦å·ã‚’åˆ¤å®šã™ã‚‹ã‹
    top_n_for_sign = 5 

    for i, pc_name in enumerate(pc_cols):
        if pc_name in sector_groups:
            target_sectors = sector_groups[pc_name]
            # æŒ‡å®šã•ã‚ŒãŸã‚°ãƒ«ãƒ¼ãƒ—ã«å±ã™ã‚‹ã‚»ã‚¯ã‚¿ãƒ¼ã®ã€ç¾åœ¨ã®è² è·é‡ã‚’å–å¾—
            current_loadings = pd.Series(components[i], index=sector_df.columns)
            group_loadings = current_loadings[current_loadings.index.isin(target_sectors)]
            
            if not group_loadings.empty:
                # ã€ã“ã“ãŒãƒã‚¤ãƒ³ãƒˆã€‘çµ¶å¯¾å€¤ãŒå¤§ãã„ä¸Šä½ n ä»¶ã®ã¿ã‚’æŠ½å‡º
                top_loadings = group_loadings.abs().sort_values(ascending=False).head(top_n_for_sign)
                # å…ƒã®ç¬¦å·ä»˜ãã®å€¤ã‚’å‚ç…§ã—ã¦å¹³å‡ã‚’è¨ˆç®—
                sign_check_value = group_loadings[top_loadings.index].mean()
                
                # ãã®ç¾¤ã®ï¼ˆæœ‰åŠ›ãªã‚»ã‚¯ã‚¿ãƒ¼ã®ï¼‰å¹³å‡ãŒè² ãªã‚‰ã€ç¬¦å·ã‚’åè»¢
                if sign_check_value < 0:
                    components[i] *= -1
                    score_df.iloc[:, i] *= -1
                # ã“ã‚Œã§ã€ã“ã®PCãŒä¸ŠãŒã‚Œã°æŒ‡å®šã‚»ã‚¯ã‚¿ãƒ¼ã‚‚ä¸ŠãŒã‚‹ã¨ã„ã†é–¢ä¿‚ãŒå›ºå®šã•ã‚Œã‚‹
        # print(f"  [PCA Fix] {pc_name} sign check finished.")
    
    # ç¬¦å·ã®åè»¢å‡¦ç†ï¼ˆè§£é‡ˆã‚’å®¹æ˜“ã«ã™ã‚‹ãŸã‚ï¼‰
    # for i in range(components.shape[0]):
    #     if np.mean(components[i]) < 0:
    #         components[i] *= -1
    #         score_df.iloc[:, i] *= -1 

    # --- è² è·é‡ï¼ˆLoadingsï¼‰ã®CSVå‡ºåŠ› ---
    loadings = pd.DataFrame(components.T, index=sector_df.columns, columns=pc_cols)
    loadings.to_csv(pca_dir / "ã‚»ã‚¯ã‚¿ãƒ¼ã®è² è·é‡_ä¸€è¦§.csv", encoding="utf-8-sig")

    pca_dir = root_dir / "pca_analysis"
    pca_dir.mkdir(exist_ok=True)

    # --- å¯„ä¸ç‡ã®ä¿å­˜ã‚’è¿½åŠ  ---
    expl = pca.explained_variance_ratio_
    expl_df = pd.DataFrame(expl.reshape(1, -1), columns=pc_cols, index=["ExplainedVariance"])
    expl_df.to_csv(pca_dir / "ä¸»æˆåˆ†_å¯„ä¸ç‡.csv", encoding="utf-8-sig")

    # ã‚¹ã‚³ã‚¢æ¨ç§»ãƒ—ãƒ­ãƒƒãƒˆï¼ˆPC3ã¾ã§ï¼‰
    plt.figure(figsize=(12, 5))
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']
    for i, pc in enumerate(pc_cols[:3]):
        plt.plot(score_df.index, score_df[pc], label=f"{pc} (å¯„ä¸:{expl[i]*100:.1f}%)", color=colors[i], lw=2)
    plt.title("ä¸»æˆåˆ†ã‚¹ã‚³ã‚¢ã®æ¨ç§» (ã‚»ã‚¯ã‚¿ãƒ¼æ¨™æº–åŒ–å¾Œã®ç”Ÿå€¤)")
    
    # --- è»¸ãƒ©ãƒ™ãƒ«è¿½åŠ  ---
    plt.xlabel("æ—¥ä»˜")
    plt.ylabel("ä¸»æˆåˆ†ã‚¹ã‚³ã‚¢")
    
    plt.axhline(0, color='black', lw=1); plt.legend(loc='upper left', bbox_to_anchor=(1, 1)); plt.grid(axis='y', alpha=0.3); plt.tight_layout()
    plt.savefig(pca_dir / "PCA_ã‚¹ã‚³ã‚¢æ¨ç§»_ç”Ÿå€¤.png"); plt.close()

    # è² è·é‡ã®æ£’ã‚°ãƒ©ãƒ•ï¼ˆPC3ã¾ã§ï¼‰
    for i, pc in enumerate(pc_cols[:3]):
        plt.figure(figsize=(10, 5))
        loadings[pc].reindex(loadings[pc].abs().sort_values(ascending=False).index).head(10).plot(kind='bar', color=colors[i])
        plt.title(f"{pc} ã‚»ã‚¯ã‚¿ãƒ¼ã®è² è·é‡ (å¯„ä¸:{expl[i]*100:.1f}%)")
        
        # --- è»¸ãƒ©ãƒ™ãƒ«è¿½åŠ  ---
        plt.xlabel("ã‚»ã‚¯ã‚¿ãƒ¼")
        plt.ylabel("è² è·é‡")
        
        plt.axhline(0, color='black', lw=1); plt.xticks(rotation=45, ha='right'); plt.grid(axis='y', alpha=0.3); plt.tight_layout()
        plt.savefig(pca_dir / f"è² è·é‡_{pc}_TOP10.png"); plt.close()

    print(f"âœ… PCAåˆ†æå®Œäº†: {pca_dir}")

def plot_abs_heatmaps(full_pca_df, output_root):
    # PCã”ã¨ã«ãƒ«ãƒ¼ãƒ—ã—ã¦å¯è¦–åŒ–
    for pc in ["PC1", "PC2", "PC3"]:
        target_df = full_pca_df[full_pca_df["PC_Type"] == pc]
        if target_df.empty: continue
        
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã—ã¦ã€ã‚»ã‚¯ã‚¿ãƒ¼åˆ—ã®ã¿æŠ½å‡ºï¼ˆæ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
        # æ–‡å­—åˆ—ã‚«ãƒ©ãƒ ã‚’é™¤å¤–ã—ã¦è»¢ç½®
        plot_data = target_df.drop(columns=["PC_Type", "Window", "Explained_Variance"]).T
        plot_data.columns = target_df["Window"] # xè»¸ã‚’ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åã«
        
        plt.figure(figsize=(14, 9))
        
        # è² è·é‡ã®çµ¶å¯¾å€¤ãªã®ã§ã€0ã‹ã‚‰ã®å¼·å¼±ãŒã¯ã£ãã‚Šã™ã‚‹ "Reds" ãªã©ã‚’æ¡ç”¨
        # vmin=0, vmax=0.5 ãªã©ã§ã‚¹ã‚±ãƒ¼ãƒ«ã‚’å›ºå®šã™ã‚‹ã¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é–“ã®æ¯”è¼ƒãŒã—ã‚„ã™ããªã‚Šã¾ã™
        sns.heatmap(plot_data, 
                    cmap="Reds", 
                    annot=False, 
                    cbar_kws={'label': f'{pc} è² è·é‡ï¼ˆçµ¶å¯¾å€¤ï¼‰'},
                    vmin=0, 
                    vmax=max(0.5, plot_data.max().max())) 
        
        # å¯„ä¸ç‡ã‚’å–å¾—
        mean_var = target_df["Explained_Variance"].mean() * 100
        plt.title(f"{pc} æ§‹æˆã‚»ã‚¯ã‚¿ãƒ¼ã®è² è·é‡æ¨ç§»ï¼ˆçµ¶å¯¾å€¤ï¼‰\n[å¹³å‡å¯„ä¸ç‡: {mean_var:.1f}%]", fontsize=14)
        plt.xlabel("ãƒ†ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ (Window)", fontsize=12)
        plt.ylabel("ã‚»ã‚¯ã‚¿ãƒ¼", fontsize=12)
        
        plt.tight_layout()
        plt.savefig(output_root / f"å…¨æœŸé–“_{pc}_æ§‹é€ æ¨ç§»_çµ¶å¯¾å€¤.png")
        plt.close()

def aggregate_results(output_root):
    output_root = Path(output_root)
    all_summaries = []
    pca_abs_details = []

    # 1. å„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’å·¡å›ã—ã¦ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’åé›†
    for window_path in sorted(output_root.glob("Window_Test_*")):
        window_name = window_path.name.replace("Window_Test_", "")
        
        # ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚µãƒãƒªãƒ¼
        summary_file = window_path / "ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚µãƒãƒªãƒ¼.csv"
        if summary_file.exists():
            df = pd.read_csv(summary_file)
            df["Test_Window"] = window_name
            all_summaries.append(df)
            
        # PCAè² è·é‡ï¼ˆçµ¶å¯¾å€¤ï¼‰
        loadings_file = window_path / "pca_analysis" / "ã‚»ã‚¯ã‚¿ãƒ¼ã®è² è·é‡_ä¸€è¦§.csv"
        variance_file = window_path / "pca_analysis" / "ä¸»æˆåˆ†_å¯„ä¸ç‡.csv"
        if loadings_file.exists() and variance_file.exists():
            ld_df = pd.read_csv(loadings_file, index_col=0)
            vr_df = pd.read_csv(variance_file, index_col=0)
            for pc in ["PC1", "PC2", "PC3"]:
                if pc in ld_df.columns:
                    abs_row = ld_df[pc].abs().to_frame().T
                    abs_row.index = [f"{window_name}_{pc}"]
                    abs_row.insert(0, "Window", window_name)
                    abs_row.insert(1, "PC_Type", pc)
                    abs_row.insert(2, "Explained_Variance", vr_df.at["ExplainedVariance", pc])
                    pca_abs_details.append(abs_row)

    if not all_summaries:
        return

    # --- 2. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã®è©³ç´°çµ±è¨ˆï¼ˆå¿˜ã‚Œã¦ãªã„ãƒã‚¤ãƒ³ãƒˆï¼š0é™¤å¤– & æ—¥æœ¬èªã‚«ãƒ©ãƒ ï¼‰ ---
    # ignore_index=True ã‚’è¿½åŠ ã—ã¦ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®é‡è¤‡ã‚’è§£æ¶ˆã™ã‚‹
    merged_summary = pd.concat(all_summaries, ignore_index=True)

    plot_final_boxplots(merged_summary, output_root)
    
    target_cols = ["è¨“ç·´RMSE", "äºˆæ¸¬RMSE", "RMSEæ¯”", "AIC", "æœ€å¤§å›ºæœ‰å€¤"]
    stats_list = []
    for col in target_cols:
        if col in merged_summary.columns:
            temp_series = merged_summary.copy()
            if col != "AIC": # AICä»¥å¤–ã¯0ã‚’å¤–ã™
                temp_series[col] = temp_series[col].replace(0, np.nan)
            
            res = temp_series.groupby("ãƒ¢ãƒ‡ãƒ«æ§‹æˆ")[col].agg([
                (f"{col}_å¹³å‡", "mean"),
                (f"{col}_æ¨™æº–åå·®", "std"),
                (f"{col}_æœ€å¤§", "max"),
                (f"{col}_æœ€å°", "min")
            ])
            stats_list.append(res)
    
    model_perf_detail = pd.concat(stats_list, axis=1)
    model_perf_detail["æœ‰åŠ¹è©¦è¡Œå›æ•°"] = merged_summary.groupby("ãƒ¢ãƒ‡ãƒ«æ§‹æˆ")["äºˆæ¸¬RMSE"].apply(lambda x: (x != 0).sum())
    model_perf_detail = model_perf_detail.round(4)
    model_perf_detail.to_csv(output_root / "å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é›†è¨ˆ_ãƒ¢ãƒ‡ãƒ«è©•ä¾¡_è©³ç´°ç‰ˆ.csv", encoding="utf-8-sig")

    # --- 3. PCAæ§‹é€ ã®é›†è¨ˆã¨ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆã“ã“ã‚‚å¿˜ã‚Œã¦ã¾ã›ã‚“ï¼ï¼‰ ---
    if pca_abs_details:
        full_pca_df = pd.concat(pca_abs_details)
        full_pca_df.to_csv(output_root / "å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é›†è¨ˆ_PCAæ§‹é€ _çµ¶å¯¾å€¤è©³ç´°.csv", encoding="utf-8-sig")
        # çµ¶å¯¾å€¤ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ä¿å­˜
        plot_abs_heatmaps(full_pca_df, output_root)

    print(f"âœ… ã™ã¹ã¦ã®é›†è¨ˆãƒ»ç”»åƒä¿å­˜ï¼ˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã€ç®±ã²ã’å›³ã€çµ±è¨ˆè©³ç´°ï¼‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

def plot_final_boxplots(merged_summary, output_root):
    """
    ã‚ãªãŸãŒã€ãˆãˆæ„Ÿã˜ã€ã¨è¨€ã£ãŸã€è‰²ä»˜ãã®ç®±ã²ã’å›³ ï¼‹ çµ±è¨ˆé‡
    """
    output_root = Path(output_root)
    plot_df_base = merged_summary.copy().reset_index(drop=True)
    
    targets = ["è¨“ç·´RMSE", "äºˆæ¸¬RMSE", "RMSEæ¯”", "AIC", "æœ€å¤§å›ºæœ‰å€¤"]
    
    for col in targets:
        if col not in plot_df_base.columns: continue
            
        plt.figure(figsize=(14, 8))
        
        # 0é™¤å¤–ï¼ˆAICä»¥å¤–ï¼‰
        plot_data = plot_df_base[plot_df_base[col] != 0].copy() if col != "AIC" else plot_df_base.copy()
        if plot_data.empty: continue

        # å¹³å‡å€¤ãŒä½ã„é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ä¸¦ã¹ã‚‹
        order = plot_data.groupby("ãƒ¢ãƒ‡ãƒ«æ§‹æˆ")[col].mean().sort_values().index
        
        # 1. ã€ãƒ¡ã‚¤ãƒ³ã€‘è‰²ä»˜ãã®ç®±ã²ã’å›³ï¼ˆã“ã‚ŒãŒã€ãˆãˆæ„Ÿã˜ã€ã®æ­£ä½“ï¼‰
        sns.boxplot(data=plot_data, x="ãƒ¢ãƒ‡ãƒ«æ§‹æˆ", y=col, order=order, 
                    palette="Set3", width=0.6, boxprops=dict(alpha=0.7))
        
        # 2. å¹³å‡å€¤(â—†)ã¨æ¨™æº–åå·®(èµ¤ç·š)ã‚’é‡ã­ã‚‹
        sns.pointplot(data=plot_data, x="ãƒ¢ãƒ‡ãƒ«æ§‹æˆ", y=col, order=order,
                      join=False, color="red", marker="D", scale=0.7, 
                      errorbar="sd", capsize=.15, label="å¹³å‡ Â± æ¨™æº–åå·®")
        
        # 3. ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆè–„ã„ç‚¹ï¼‰ã¯ãƒã‚¤ã‚ºã«ãªã‚‹ã®ã§ã€ã“ã“ã§ã¯é™¤å¤–ã‹æ¥µè–„ã«
        # sns.stripplot(data=plot_data, x="ãƒ¢ãƒ‡ãƒ«æ§‹æˆ", y=col, order=order, color="black", alpha=0.1, jitter=True)

        plt.xticks(rotation=45, ha='right')
        plt.title(f"{col}ã®ç®±ã²ã’å›³", fontsize=14)
        plt.grid(axis='y', linestyle='--', alpha=0.4)
        plt.legend(loc='upper left')
        plt.tight_layout()
        
        plt.savefig(output_root / f"å…¨æœŸé–“_ç®±ã²ã’_{col}.png", dpi=300)
        plt.close()

def visualize_model_performance(csv_path):
    # CSVï¼ˆè©³ç´°é›†è¨ˆç‰ˆï¼‰ã®èª­ã¿è¾¼ã¿
    df = pd.read_csv(csv_path, index_col=0)
    output_dir = Path(csv_path).parent
    plt.style.use('ggplot')

    # å‡ºåŠ›ã—ãŸã„æŒ‡æ¨™ã®ãƒªã‚¹ãƒˆ
    targets = ["äºˆæ¸¬RMSE", "AIC", "è¨“ç·´RMSE", "æœ€å¤§å›ºæœ‰å€¤", "RMSEæ¯”"]

    for col in targets:
        avg_col = f"{col}_å¹³å‡"
        std_col = f"{col}_æ¨™æº–åå·®"
        
        if avg_col not in df.columns:
            continue

        # --- 1. æ£’ã‚°ãƒ©ãƒ• ï¼‹ ã‚¨ãƒ©ãƒ¼ãƒãƒ¼ï¼ˆå¹³å‡ã¨æ¨™æº–åå·®ï¼‰ ---
        plt.figure(figsize=(12, 7))
        # å¹³å‡å€¤ãŒä½ã„é †ï¼ˆè‰¯ã„é †ï¼‰ã«ä¸¦ã¹æ›¿ãˆ
        df_sorted = df.sort_values(avg_col)
        
        # ã‚ãªãŸãŒã€Œã„ã„æ„Ÿã˜ã€ã¨è¨€ã£ã¦ãã‚ŒãŸã‚¹ã‚¿ã‚¤ãƒ«
        plt.bar(df_sorted.index, df_sorted[avg_col], 
                yerr=df_sorted[std_col], 
                capsize=5, color='skyblue', edgecolor='navy', alpha=0.7)
        
        plt.xticks(rotation=45, ha='right')
        plt.ylabel(f"{col} (å¹³å‡)")
        plt.title(f"ãƒ¢ãƒ‡ãƒ«æ§‹æˆåˆ¥ {col} æ¯”è¼ƒ\n(ã‚¨ãƒ©ãƒ¼ãƒãƒ¼ã¯æ¨™æº–åå·®ã‚’è¡¨ç¤º)")
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.tight_layout()
        
        # æŒ‡æ¨™åã‚’ã¤ã‘ã¦ä¿å­˜
        # plt.savefig(output_dir / f"å…¨æœŸé–“_æ¯”è¼ƒ_{col}.png", dpi=300)
        plt.close()

    # --- 2. AIC vs äºˆæ¸¬RMSE ã®æ•£å¸ƒå›³ï¼ˆã“ã‚Œã¯1æšã ã‘ä½œæˆï¼‰ ---
    if "AIC_å¹³å‡" in df.columns and "äºˆæ¸¬RMSE_å¹³å‡" in df.columns:
        plt.figure(figsize=(10, 7))
        sns.scatterplot(data=df, x="AIC_å¹³å‡", y="äºˆæ¸¬RMSE_å¹³å‡", 
                        size="äºˆæ¸¬RMSE_æ¨™æº–åå·®", hue=df.index, 
                        sizes=(100, 1000), alpha=0.6)
        
        for i, txt in enumerate(df.index):
            plt.annotate(txt, (df["AIC_å¹³å‡"].iloc[i], df["äºˆæ¸¬RMSE_å¹³å‡"].iloc[i]), 
                         xytext=(5, 5), textcoords='offset points', fontsize=8)
            
        plt.title("ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•(AIC) vs äºˆæ¸¬ç²¾åº¦(RMSE)")
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(output_dir / "å…¨æœŸé–“_AIC_vs_RMSE_åˆ†å¸ƒ.png", dpi=300)
        plt.close()

    print(f"ğŸ’¾ ã‚°ãƒ©ãƒ•ï¼ˆæ£’ã‚°ãƒ©ãƒ•5ç¨® ï¼‹ æ•£å¸ƒå›³1ç¨®ï¼‰ã‚’ {output_dir} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

# =========================================================
# 4. å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é€£çµäºˆæ¸¬ã‚°ãƒ©ãƒ•ã®ä½œæˆ
# =========================================================
def plot_connected_forecasts(output_root, df_raw, meta):
    import matplotlib.style
    # èƒŒæ™¯ã‚’ç™½ã«å›ºå®šã—ã€ã‚°ãƒ©ãƒ•ã®ä½“è£ã‚’æ•´ãˆã‚‹
    plt.style.use('default') 
    plt.rcParams['font.family'] = ["Hiragino Sans"]
    
    output_root = Path(output_root)
    m_cols_transformed = list(meta.keys())
    
    # 4ã¤ã®é–‹å§‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´
    all_preds = {m: {p: [] for p in range(4)} for m in m_cols_transformed}
    rmse_list = {m: {p: [] for p in range(4)} for m in m_cols_transformed}

    window_dirs = sorted(output_root.glob("Window_Test_*"))
    
    for i, window_path in enumerate(window_dirs):
        phase = i % 4
        
        # æ¢ç´¢ç¯„å›²ã‚’åºƒã’ã‚‹ï¼šã™ã¹ã¦ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ãã®å¤‰æ•°ã®CSVã‚’æ¢ã™
        for sub in window_path.iterdir():
            if not sub.is_dir(): continue
            
            for m_key, m_info in meta.items():
                orig_name = m_info["orig"]
                # ã€Œäºˆæ¸¬å€¤_æ°´æº–ãƒ™ãƒ¼ã‚¹_å¤‰æ•°å.csvã€ã‚’ãƒ”ãƒ³ãƒã‚¤ãƒ³ãƒˆã§æ¢ã™
                target_csv = sub / f"äºˆæ¸¬å€¤_æ°´æº–ãƒ™ãƒ¼ã‚¹_{orig_name}.csv"
                
                if target_csv.exists():
                    pdf = pd.read_csv(target_csv, index_col=0, parse_dates=True)
                    col_name = f"{orig_name}_Pred"
                    if col_name in pdf.columns:
                        preds = pdf[col_name]
                        all_preds[m_key][phase].append(preds)
                        # RMSEè¨ˆç®—
                        actual = df_raw.loc[preds.index, orig_name]
                        if not actual.dropna().empty:
                            rmse = np.sqrt(np.mean((actual - preds)**2))
                            rmse_list[m_key][phase].append(rmse)

    # ã‚°ãƒ©ãƒ•ä½œæˆ
    for m_col, m_info in meta.items():
        orig_name = m_info["orig"]
        if not any(all_preds[m_col].values()): continue # ãƒ‡ãƒ¼ã‚¿ãŒä¸€ã¤ã‚‚ãªã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—

        fig, ax = plt.subplots(figsize=(14, 7), facecolor='white')
        ax.set_facecolor('white') # ã‚°ãƒ©ãƒ•ã‚¨ãƒªã‚¢ã‚‚ç™½
        
        # å®Ÿç¸¾å€¤ï¼ˆå®Ÿç·šãƒ»é»’ï¼‰
        ax.plot(df_raw.index, df_raw[orig_name], color='black', lw=2, label="å®Ÿç¸¾å€¤", zorder=2)
        
        colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3']
        rmse_summary = []
        
        for p in range(4):
            if not all_preds[m_col][p]: continue
            
            combined = pd.concat(all_preds[m_col][p]).sort_index()
            combined = combined[~combined.index.duplicated(keep='last')]
            
            avg_rmse = np.mean(rmse_list[m_col][p]) if rmse_list[m_col][p] else 0
            rmse_summary.append(f"P{p+1}:{avg_rmse:.3f}")
            
            # äºˆæ¸¬å€¤ï¼ˆç‚¹ç·šï¼‰
            ax.plot(combined.index, combined.values, color=colors[p], linestyle='--', 
                    marker='o', markersize=4, alpha=0.9, 
                    label=f"ãƒ‘ã‚¿ãƒ¼ãƒ³{p+1} (RMSE: {avg_rmse:.3f})", zorder=3)

        # å‡¡ä¾‹ã¨ã‚¿ã‚¤ãƒˆãƒ«ã®è¨­å®š
        total_avg = np.mean([np.mean(v) for v in rmse_list[m_col].values() if v])
        ax.set_title(f"å…¨æœŸé–“é€£çµäºˆæ¸¬: {orig_name}\nã€RMSEã€‘{' / '.join(rmse_summary)} (å¹³å‡: {total_avg:.3f})", 
                     fontsize=14, pad=20)
        
        ax.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=True, facecolor='white')
        ax.grid(True, color='gray', linestyle=':', alpha=0.3)
        ax.set_ylabel("æ°´æº– (Level)")
        ax.set_xlabel("å¹´æœˆ")
        
        # Xè»¸ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        ax.xaxis.set_major_locator(mpl.dates.MonthLocator(bymonth=[1, 4, 7, 10]))
        ax.xaxis.set_major_formatter(mpl.dates.DateFormatter('%Y-%m'))
        
        plt.tight_layout()
        plt.savefig(output_root / f"å…¨æœŸé–“é€£çµäºˆæ¸¬_{orig_name}.png", dpi=200, facecolor='white')
        plt.close()

import matplotlib.dates as mdates

def plot_exog_trends(csv_path, output_dir, target_var="GDP"):
    output_root = Path(output_dir)
    df = pd.read_csv(csv_path)
    
    sub_df = df[(df["Target_Variable"] == target_var) & 
                (df["Model_Type"].str.contains("PC"))].copy()
    
    # --- ã€ã“ã“ãŒãƒã‚¤ãƒ³ãƒˆï¼šæ—¥ä»˜ã¸ã®å¤‰æ›ã€‘ ---
    # Windowåã®æœ«å°¾ï¼ˆãƒ†ã‚¹ãƒˆæœŸé–“ã®çµ‚äº†æ—¥ï¼‰ã‚’æ—¥ä»˜å‹ã«å¤‰æ›ã™ã‚‹
    sub_df["EndDateStr"] = sub_df["Window"].str.split("_").str[-1]
    sub_df["Date"] = pd.to_datetime(sub_df["EndDateStr"], format='%Y%m%d')
    sub_df = sub_df.sort_values("Date")
    
    models = sub_df["Model_Type"].unique()
    pc_cols = [c for c in sub_df.columns if "PC" in c]

    fig, axes = plt.subplots(len(models), 1, figsize=(12, 4 * len(models)), sharex=True)
    if len(models) == 1: axes = [axes]

    for ax, m_type in zip(axes, models):
        m_data = sub_df[sub_df["Model_Type"] == m_type]
        active_pcs = [c for c in pc_cols if not m_data[c].isna().all()]
        
        for col in active_pcs:
            # Xè»¸ã«æ–‡å­—åˆ—ã§ã¯ãªãã€ŒDateã€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¸¡ã™
            ax.plot(m_data["Date"], m_data[col], marker='o', label=col, lw=2)
            
        ax.set_title(f"Target: {target_var} | Model: {m_type}", fontsize=12, fontweight='bold')
        ax.axhline(0.3, color='red', ls='--', alpha=0.6, label="LB: 0.3")
        ax.axhline(0, color='black', lw=1, alpha=0.3)
        
        # --- ã€ã“ã“ãŒãƒã‚¤ãƒ³ãƒˆï¼šXè»¸ã®ç›®ç››ã‚Šã‚’ç¶ºéº—ã«ã™ã‚‹ã€‘ ---
        ax.xaxis.set_major_locator(mdates.YearLocator()) # 1å¹´åˆ»ã¿
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y/%m')) # è¡¨ç¤ºå½¢å¼
        
        ax.set_ylabel("Coefficient")
        ax.legend(loc='upper right', fontsize=9)
        ax.grid(axis='both', alpha=0.2)

    plt.xticks(rotation=45)
    plt.tight_layout()
    
    save_path = output_root / f"ä¿‚æ•°æ¨ç§»_{target_var}_æ—¥ä»˜ä¿®æ­£ç‰ˆ.png"
    plt.savefig(save_path, dpi=200, facecolor='white')
    plt.close()
    print(f"âœ… æ—¥ä»˜ã‚’ä¿®æ­£ã—ã¦ä¿å­˜ã—ã¾ã—ãŸ: {save_path}")

# =========================================================
# 3. ãƒ¡ã‚¤ãƒ³å‡¦ç†
# =========================================================
def main():
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†ï¼ˆã“ã“ã¯å…±é€šï¼‰
    df_raw = pd.read_csv(CONFIG["data_path"], index_col=0, parse_dates=True).sort_index()
    sector_df, macro_df, common_idx, meta = prepare_aligned(df_raw)
    
    # --- Moving Window è¨­å®š ---
    test_len = 4
    n_total = len(common_idx)
    all_coefficients_records=[]
    
    # ãƒ†ã‚¹ãƒˆæœŸé–“ã‚’1æœŸãšã¤ãšã‚‰ã—ã¦å…¨çµ„ã¿åˆã‚ã›ã‚’å®Ÿè¡Œ
    for i in range(n_total - test_len + 1):
        # ãƒ†ã‚¹ãƒˆæœŸé–“ã¨è¨“ç·´æœŸé–“ï¼ˆãƒ†ã‚¹ãƒˆä»¥å¤–ã™ã¹ã¦ï¼‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        te_idx_list = np.arange(i, i + test_len)
        tr_idx_list = np.setdiff1d(np.arange(n_total), te_idx_list)
        
        # ãƒ•ã‚©ãƒ«ãƒ€åç”¨ã®æ—¥ä»˜å–å¾—
        d_start = common_idx[te_idx_list[0]]
        d_end = common_idx[te_idx_list[-1]]
        
        # ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®šï¼ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã”ã¨ã«ä½œæˆï¼‰
        root_dir = Path(CONFIG["output_dir"]) / f"Window_Test_{d_start:%Y%m%d}_{d_end:%Y%m%d}"
        root_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"â–¶ï¸ å®Ÿè¡Œä¸­: {root_dir.name}")

        # 1. PCA: è¨“ç·´ãƒ‡ãƒ¼ã‚¿(tr_idx_list)ã§Fitã—ã€å…¨æœŸé–“ã‚’Transform
        sc_pca = StandardScaler()
        X_tr_pca = sc_pca.fit_transform(sector_df.iloc[tr_idx_list])
        pca_temp = PCA().fit(X_tr_pca)
        n_pcs = max(3, np.argmax(np.cumsum(pca_temp.explained_variance_ratio_) >= 0.90) + 1)
        
        pca = PCA(n_components=n_pcs).fit(X_tr_pca)
        pc_cols_orig = [f"PC{k+1}" for k in range(n_pcs)]
        
        # 2. å…¨æœŸé–“ã®PCå·®åˆ†ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆãƒªãƒ¼ã‚¯é˜²æ­¢ã®ãŸã‚PCAã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¯ç¶­æŒï¼‰
        # pca_no_leakç›¸å½“ã®å‡¦ç†ã‚’Windowç”¨ã«èª¿æ•´
        full_pcs = pca.transform(sc_pca.transform(sector_df))
        pc_diff_all = pd.DataFrame(full_pcs, index=sector_df.index).diff().dropna()
        pc_cols_diff = [f"PC{k+1}_DIFF" for k in range(3)]
        pc_diff_all = pc_diff_all.iloc[:, :3]
        pc_diff_all.columns = pc_cols_diff

        # --- å…ƒã®åˆ†æãƒ¬ãƒãƒ¼ãƒˆ(analyze_pca_details)å®Ÿè¡Œ ---
        # å¼•æ•°ã¯Windowã«åˆã‚ã›ã¦èª¿æ•´
        analyze_pca_details(pca, sector_df, pc_cols_orig, root_dir, tr_idx_list[0], tr_idx_list[-1], test_len)
        
        # ãƒã‚¯ãƒ­åŒæœŸ
        m_df_win = macro_df.loc[pc_diff_all.index]
        combinations = [[]] + [list(c) for r in range(1, 4) for c in itertools.combinations(pc_cols_diff, r)]
        summary_list = []

        for combo in combinations:
            m_name_display = ", ".join(combo) if combo else "ãƒã‚¯ãƒ­ã®ã¿"
            sub_dir = root_dir / ("_".join(combo) if combo else "BASE_VAR_ONLY")
            sub_dir.mkdir(parents=True, exist_ok=True)
            
            # --- è¨“ç·´ã¨ãƒ†ã‚¹ãƒˆã®åˆ‡ã‚Šå‡ºã— ---
            win_idx = m_df_win.index
            tr_pre_dates = common_idx[tr_idx_list[tr_idx_list < te_idx_list[0]]]
            tr_post_dates = common_idx[tr_idx_list[tr_idx_list > te_idx_list[-1]]]
            te_dates = common_idx[te_idx_list]
            
            # 1. éå»å´ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿
            tr_pre = pd.concat([m_df_win.loc[win_idx.isin(tr_pre_dates)], 
                                pc_diff_all.loc[win_idx.isin(tr_pre_dates), combo]], axis=1)
            
            # 2. æœªæ¥å´ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿
            tr_post = pd.concat([m_df_win.loc[win_idx.isin(tr_post_dates)], 
                                 pc_diff_all.loc[win_idx.isin(tr_post_dates), combo]], axis=1)
            
            # ã€é‡è¦ã€‘æœªæ¥å´ãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­1è¡Œã‚’NaNã«ã™ã‚‹ï¼ˆãƒ©ã‚°è¨ˆç®—ã§éå»å´ã¨ç¹‹ãŒã‚‹ã®ã‚’é˜²ããŸã‚ï¼‰
            if not tr_post.empty:
                tr_post.iloc[0, :] = np.nan

            # 3. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®çµåˆï¼ˆä¸­æŠœãã•ã‚ŒãŸç®‡æ‰€ã«NaNã®å£ãŒã§ãã‚‹ï¼‰
            tr_raw = pd.concat([tr_pre, tr_post])
            te_raw = pd.concat([m_df_win.loc[win_idx.isin(te_dates)], 
                                pc_diff_all.loc[win_idx.isin(te_dates), combo]], axis=1)

            means, stds = tr_raw.mean(), tr_raw.std(ddof=0).replace(0, 1.0)
            tr_s, te_s = (tr_raw - means) / stds, (te_raw - means) / stds
            m_cols, m_dim = list(macro_df.columns), len(macro_df.columns)
            
            # ãƒ¢ãƒ‡ãƒ«æ¨å®š
            Y_tr, X_tr = make_design(tr_s[m_cols], tr_s[combo] if combo else None, CONFIG["p_lag"])
            # --- ä¿®æ­£ç‰ˆï¼šRidgeã¨ç¬¦å·åˆ¶ç´„ã‚’åŒæ™‚ã«é©ç”¨ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ ---
            # --- ä¿®æ­£ç‰ˆï¼šä¿‚æ•°ç¯„å›²ã®å³æ ¼åŒ–ã¨CSVä¿å­˜ (ã‚¨ãƒ©ãƒ¼è§£æ¶ˆç‰ˆ) ---

            # 1. ã™ã¹ã¦ã®åˆ¶ç´„ã‚’ã“ã“ã«é›†ç´„ (ç¯„å›²æŒ‡å®š or ç¬¦å·æŒ‡å®š)
            STRICT_CONSTRAINTS = {
                ("PC1", "GDP"): (0.3, 0.7),
                ("PC1", "NIKKEI"): (0.3, 0.7),
                ("PC1", "CPI"): (0.1, 0.5),
                ("PC1", "UNEMP_RATE"): (-0.7, -0.3),
                # ä»¥å‰ã® BASE_SIGN_CONSTRAINTS ç›¸å½“ã‚‚ã“ã“ã«æ›¸ã„ã¦ãŠã‘ã°ã‚¨ãƒ©ãƒ¼ã«ãªã‚Šã¾ã›ã‚“
                ("*", "UNEMP"): (-np.inf, -0.3), # å¿…è¦ãªã‚‰è¿½åŠ 
            }

            if CONFIG.get("use_constraints", True):
                beta_list = []
                ridge_aug = np.sqrt(CONFIG["ridge"]) * np.eye(X_tr.shape[1])
                X_aug = np.vstack([X_tr, ridge_aug])
                
                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åãƒªã‚¹ãƒˆ
                param_names = ["CONST"] + [f"LAG_{m}" for m in m_cols] + list(combo)
                
                for j, target_m_col in enumerate(m_cols):
                    orig_target = meta[target_m_col]["orig"]
                    Y_aug = np.concatenate([Y_tr[:, j], np.zeros(X_tr.shape[1])])
                    lb = np.full(X_aug.shape[1], -np.inf)
                    ub = np.full(X_aug.shape[1], np.inf)
                    
                    # --- ä¿®æ­£ç‰ˆï¼šãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰å¯¾å¿œã®åˆ¶ç´„é©ç”¨ãƒ­ã‚¸ãƒƒã‚¯ ---
                    if combo:
                        start_exog_idx = 1 + m_dim
                        for k, exog_pc_name in enumerate(combo):
                            orig_exog = exog_pc_name.split("_")[0]
                            target_idx = start_exog_idx + k
                            
                            # å„ªå…ˆé †ä½ã‚’ã¤ã‘ã¦åˆ¶ç´„ã‚’æ¢ã™
                            # 1. å€‹åˆ¥æŒ‡å®š ("PC1", "GDP")
                            # 2. å¤–ç”Ÿå¤‰æ•°å…¨æŒ‡å®š ("*", "GDP")
                            # 3. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå…¨æŒ‡å®š ("PC1", "*")
                            # 4. å…¨æŒ‡å®š ("*", "*")
                            bound = (STRICT_CONSTRAINTS.get((orig_exog, orig_target)) or 
                                    STRICT_CONSTRAINTS.get(("*", orig_target)) or 
                                    STRICT_CONSTRAINTS.get((orig_exog, "*")) or 
                                    STRICT_CONSTRAINTS.get(("*", "*")))

                            if bound:
                                lb[target_idx] = bound[0]
                                ub[target_idx] = bound[1]
                            # --- ã‚¨ãƒ©ãƒ¼ã®å…ƒã ã£ãŸ else ç¯€ (old_c) ã¯å‰Šé™¤ã—ã¾ã—ãŸ ---

                    res = lsq_linear(X_aug, Y_aug, bounds=(lb, ub), lsmr_tol='auto')
                    beta_list.append(res.x)

                    # CSVå‡ºåŠ›ç”¨ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ä½œæˆ
                    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åã¯ [CONST, LAG_GDP, ..., PC1_DIFF, ...] ã®é †
                    param_names = ["CONST"] + [f"LAG_{m}" for m in m_cols] + list(combo)

                    record = {
                        "Window": root_dir.name,
                        "Model_Type": m_name_display,
                        "Target_Variable": orig_target,
                    }
                    # å„ä¿‚æ•°ã‚’ã‚«ãƒ©ãƒ ã¨ã—ã¦è¿½åŠ 
                    for name, val in zip(param_names, res.x):
                        record[name] = val

                    # mainã®å†’é ­ã§å®šç¾©ã—ãŸ all_coefficients_records ã«è¿½åŠ 
                    all_coefficients_records.append(record)

                Beta = np.array(beta_list).T

            # --- æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«æ¨å®šç›´å¾Œã‹ã‚‰ ---
            # æŒ‡æ¨™è¨ˆç®—
            tr_resid = Y_tr - X_tr @ Beta
            n_obs = len(Y_tr)
            tr_rmse = np.sqrt(np.mean(tr_resid**2))
            
            Y_te, X_te = make_design(te_s[m_cols], te_s[combo] if combo else None, CONFIG["p_lag"])
            te_rmse = np.sqrt(np.mean((Y_te - X_te @ Beta)**2)) if len(Y_te) > 0 else np.nan
            
            # --- AICè¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ã®å¼·åŒ–ç‰ˆ ---
            sigma_matrix = (tr_resid.T @ tr_resid) / n_obs
            sign, logdet = np.linalg.slogdet(sigma_matrix)
            
            if sign > 0:
                # é€šå¸¸ã®è¨ˆç®—
                aic_val = n_obs * logdet + 2 * Beta.size
            else:
                # è¡Œåˆ—å¼ãŒæ­£å¸¸ã«è¨ˆç®—ã§ããªã„å ´åˆã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
                # æ­£ã®å›ºæœ‰å€¤ã®ã¿ã‚’æŠ½å‡ºã—ã¦å¯¾æ•°å’Œã‚’ã¨ã‚‹
                eigs = np.linalg.eigvalsh(sigma_matrix)
                valid_eigs = eigs[eigs > 1e-15] # ã»ã¼ã‚¼ãƒ­ä»¥ä¸Šã®å›ºæœ‰å€¤ã®ã¿
                if len(valid_eigs) > 0:
                    aic_val = n_obs * np.sum(np.log(valid_eigs)) + 2 * Beta.size
                else:
                    aic_val = np.nan
            # -------------------------------

            A1 = Beta[1:1+m_dim, :].T
            max_eig_abs = np.max(np.abs(np.linalg.eigvals(A1)))

            summary_list.append({
                "ãƒ¢ãƒ‡ãƒ«æ§‹æˆ": m_name_display,
                "è¨“ç·´RMSE": round(tr_rmse, 4),
                "äºˆæ¸¬RMSE": round(te_rmse, 4) if not np.isnan(te_rmse) else np.nan,
                "RMSEæ¯”": round(te_rmse / tr_rmse, 2) if tr_rmse > 0 and not np.isnan(te_rmse) else np.nan,
                "AIC": round(aic_val, 2) if not np.isnan(aic_val) else np.nan,
                "æœ€å¤§å›ºæœ‰å€¤": round(max_eig_abs, 3)
            })

            # --- äºˆæ¸¬çµæœã®å¯è¦–åŒ– (é€æ¬¡äºˆæ¸¬ & å…¨ä½“è¡¨ç¤ºç‰ˆ) ---
            # 1. é€æ¬¡äºˆæ¸¬ (Dynamic Forecast) ã®å®Ÿè¡Œ
            y_te_pred_list = []
            if len(X_te) > 0:
                # æœ€åˆã®å…¥åŠ›ï¼ˆãƒ†ã‚¹ãƒˆæœŸé–“1ç‚¹ç›®ã®ãŸã‚ã®ãƒ©ã‚°ã‚’å«ã‚€ï¼‰
                curr_X = X_te[0:1, :] 
                
                for t in range(CONFIG["test_steps"]):
                    # ç¾åœ¨ã®å…¥åŠ›ã§äºˆæ¸¬
                    pred_t = curr_X @ Beta  # shape: (1, m_dim)
                    y_te_pred_list.append(pred_t)
                    
                    if t < CONFIG["test_steps"] - 1:
                        # æ¬¡ã®äºˆæ¸¬ã®ãŸã‚ã®å…¥åŠ›ã‚’æ§‹ç¯‰
                        # [å®šæ•°é …(1), ä»Šå›ã®äºˆæ¸¬å€¤(ãƒ©ã‚°1), å¤–ç”Ÿå¤‰æ•°(PC)ã®å®Ÿéš›å€¤]
                        next_lag_y = pred_t
                        # PCï¼ˆå¤–ç”Ÿå¤‰æ•°ï¼‰ã¯å®Ÿç¸¾ãŒã‚ã‚‹ç¯„å›²ã§ä½¿ã†ï¼ˆãªã‘ã‚Œã°0ã‹æœ€å¾Œã®å€¤ï¼‰
                        if combo:
                            next_exog = te_s[combo].iloc[t+1:t+2].values if (t+1) < len(te_s) else te_s[combo].iloc[-1:].values
                        else:
                            next_exog = np.empty((1, 0))
                        
                        curr_X = np.concatenate([[[1.0]], next_lag_y, next_exog], axis=1)

                y_te_pred_scaled = np.vstack(y_te_pred_list)
            else:
                y_te_pred_scaled = None

            # --- äºˆæ¸¬çµæœã®å¯è¦–åŒ– (æ—¥æœ¬èªåŒ– & i=0 ã‚¹ã‚­ãƒƒãƒ—ç‰ˆ) ---
            
            # --- äºˆæ¸¬çµæœã®å¯è¦–åŒ– (é€æ¬¡äºˆæ¸¬ãƒ»æ—¥ä»˜å›ºå®šãƒ»æ—¥æœ¬èªç‰ˆ) ---

            # i=0 (æœ€åˆæœŸ) ã‚’é™¤å¤–ã—ã€ã‹ã¤äºˆæ¸¬å€¤ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å®Ÿè¡Œ
            if y_te_pred_scaled is not None and i != 0:
                
                # 1. äºˆæ¸¬æœŸé–“ã®æ—¥ä»˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
                start_d_idx = np.where(common_idx == te_dates[0])[0][0]
                te_actual_dates = common_idx[start_d_idx : start_d_idx + CONFIG["test_steps"]]
                
                # 2. è¡¨ç¤ºç¯„å›² (éå»ã®å®Ÿç¸¾ã‚‚è¦‹ãˆã‚‹ã‚ˆã†ã«å‰å¾Œã‚’èª¿æ•´)
                plot_start_idx = max(0, start_d_idx - 8) 
                plot_end_idx = min(len(common_idx), start_d_idx + CONFIG["test_steps"] + 4)
                full_display_range = common_idx[plot_start_idx:plot_end_idx]

                for i_m, m_col in enumerate(m_cols):
                    orig_name = meta[m_col]["orig"]
                    meth = meta[m_col]["method"]
                    
                    # 3. æ°´æº–å¾©å…ƒãƒ­ã‚¸ãƒƒã‚¯
                    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°è§£é™¤
                    pred_change = (y_te_pred_scaled[:, i_m] * stds[m_col]) + means[m_col]
                    # i != 0 ãªã®ã§å¿…ãšç›´å‰ã®å®Ÿç¸¾(ãƒ©ã‚°)ãŒå–å¾—å¯èƒ½
                    last_actual_level = df_raw.loc[:te_actual_dates[0], orig_name].iloc[-2]
                    
                    pred_levels = []
                    curr_level = last_actual_level
                    for val in pred_change:
                        if meth == "LOGDIFF":
                            curr_level = curr_level * np.exp(val)
                        else:
                            curr_level = curr_level + val
                        pred_levels.append(curr_level)
                    
                    # --- äºˆæ¸¬å€¤ã®è¨ˆç®—ãŒçµ‚ã‚ã£ãŸç›´å¾Œã«é…ç½® ---
                    p_df = pd.DataFrame(pred_levels, index=te_actual_dates, columns=[f"{orig_name}_Pred"])
                    # å¤‰æ•°åä»˜ãã§ä¿å­˜ã™ã‚‹ã“ã¨ã§ã€å¾Œã®é›†è¨ˆã‚’ç¢ºå®Ÿã«ã—ã¾ã™
                    p_df.to_csv(sub_dir / f"äºˆæ¸¬å€¤_æ°´æº–ãƒ™ãƒ¼ã‚¹_{orig_name}.csv", encoding="utf-8-sig")

                    # --- 4. ã‚°ãƒ©ãƒ•æç”» ---
                    plt.figure(figsize=(11, 5.5))
                    
                    # å®Ÿç¸¾å€¤ (é»’) ã¨ äºˆæ¸¬å€¤ (èµ¤)
                    plt.plot(full_display_range, df_raw.loc[full_display_range, orig_name], 
                             label="å®Ÿç¸¾å€¤", color="#333333", marker='o', markersize=5, alpha=0.8)
                    plt.plot(te_actual_dates, pred_levels, 
                             label="äºˆæ¸¬å€¤ (é€æ¬¡)", color="red", linestyle="--", marker="x", markersize=7, lw=2)
                    
                    # ãƒ†ã‚¹ãƒˆæœŸé–“ã®èƒŒæ™¯
                    plt.axvspan(te_actual_dates[0], te_actual_dates[-1], color='gray', alpha=0.1, label='äºˆæ¸¬å¯¾è±¡æœŸé–“')
                    
                    # --- 5. æ—¥ä»˜ç›®ç››ã‚Šã®å›ºå®š (3, 6, 9, 12æœˆã®ã¿è¡¨ç¤º) ---
                    ax = plt.gca()
                    # å››åŠæœŸæœ«ã«ç›®ç››ã‚Šã‚’å¼·åˆ¶
                    ax.xaxis.set_major_locator(mpl.dates.MonthLocator(bymonth=[3, 6, 9, 12]))
                    ax.xaxis.set_major_formatter(mpl.dates.DateFormatter('%Yå¹´%mæœˆ'))
                    
                    plt.title(f"ã€äºˆæ¸¬ã€‘{orig_name} : {m_name_display}", fontsize=14)
                    plt.xlabel("å¹´æœˆ", fontsize=12)
                    plt.ylabel("æ°´æº– (Level)", fontsize=12)
                    plt.legend(loc='best', frameon=True, shadow=True)
                    plt.grid(True, which='major', linestyle='--', alpha=0.4)
                    plt.tight_layout()
                    
                    # ä¿å­˜
                    plt.savefig(sub_dir / f"PRED_{orig_name}.png", dpi=150)
                    plt.close()

            # --- å…ƒã®IRFãƒ—ãƒ­ãƒƒãƒˆ(å¤‰æ›´ãªã—) ---
            if CONFIG["do_irf"] and combo:
                B = Beta[1+m_dim:, :].T
                for j, pc_label in enumerate(combo):
                    pc_folder = sub_dir / f"Shock_{pc_label}"; pc_folder.mkdir(parents=True, exist_ok=True)
                    impact = np.zeros((13, m_dim)); impact[1] = B[:, j]
                    for h in range(2, 13): impact[h] = A1 @ impact[h-1]
                    for i_m, m_col in enumerate(m_cols):
                        orig_m, meth_m = meta[m_col]["orig"], meta[m_col]["method"]
                        base = BASE_LEVELS.get(orig_m, 100); imp_raw = impact[:, i_m] * stds[m_col]
                        vals = [base]
                        for s in range(1, 13): vals.append(vals[-1] * np.exp(imp_raw[s]) if meth_m == "LOGDIFF" else vals[-1] + imp_raw[s])
                        plt.figure(figsize=(6, 3.5))
                        ax = plt.gca()
                        ax.yaxis.get_major_formatter().set_useOffset(False)
                        ax.yaxis.get_major_formatter().set_scientific(False)
                        plt.plot(range(13), vals, color='k', lw=1)
                        plt.scatter(range(13), vals, marker='o', s=15, color='k', zorder=2)
                        plt.scatter(1, vals[1], facecolors='none', edgecolors='k', marker='o', s=100, lw=1, zorder=3)
                        plt.axvline(x=1, color='k', ls='--', lw=0.7, alpha=0.6)
                        plt.axhline(base, color='gray', ls=':', lw=0.8)
                        plt.title(f"{pc_label} â†’ {orig_m}")
                        plt.grid(alpha=0.15); plt.tight_layout()
                        plt.savefig(pc_folder / f"{orig_m}_å¿œç­”.png"); plt.close()

        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã”ã¨ã®ã‚µãƒãƒªãƒ¼ä¿å­˜
        pd.DataFrame(summary_list).sort_values("AIC").to_csv(root_dir / "ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚µãƒãƒªãƒ¼.csv", index=False, encoding="utf-8-sig")

    # --- å®Ÿè¡Œéƒ¨åˆ† ---
    # 1. å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®çµ±è¨ˆãƒ»ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—é›†è¨ˆ
    aggregate_results(CONFIG["output_dir"])

    output_folder = Path(CONFIG["output_dir"])
    csv_file = output_folder / "å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é›†è¨ˆ_ãƒ¢ãƒ‡ãƒ«è©•ä¾¡_è©³ç´°ç‰ˆ.csv"
    
    # 2. ç®±ã²ã’å›³ã‚„æ£’ã‚°ãƒ©ãƒ•ãªã©ã®è©•ä¾¡å¯è¦–åŒ–
    if csv_file.exists():
        visualize_model_performance(csv_file)
    
    # 3. â˜…ã“ã“ã§å®Ÿè¡Œï¼šå…¨æœŸé–“é€£çµäºˆæ¸¬ã‚°ãƒ©ãƒ•ã®ç”Ÿæˆ
    print(f"ğŸ“Š å…¨æœŸé–“ã®é€£çµäºˆæ¸¬ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆä¸­...")
    plot_connected_forecasts(CONFIG["output_dir"], df_raw, meta)

    # --- [ä¿®æ­£ç®‡æ‰€] main() ã®æœ€å¾Œ ---
    if all_coefficients_records:
        df_coef = pd.DataFrame(all_coefficients_records)
        # ä¿‚æ•°ãŒ 0.3 ã‚„ 0.7 ã«å¼µã‚Šä»˜ã„ã¦ã„ã‚‹ã‹ç¢ºèªã—ã‚„ã™ãã™ã‚‹ãŸã‚
        output_path = Path(CONFIG["output_dir"]) / "all_model_coefficients.csv"
        df_coef.to_csv(output_path, index=False, encoding="utf-8-sig")
        print(f"\nâœ… å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ä¿‚æ•°è©³ç´°ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
    
    print(f"âœ… å…¨å·¥ç¨‹å®Œäº†ã€‚{CONFIG['output_dir']} å†…ã®ç”»åƒã¨CSVã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    # ä¿‚æ•°è¡Œåˆ—
    # mainã®ä¸­ã§ã®å‘¼ã³å‡ºã—ä¾‹
    output_path = Path(CONFIG["output_dir"]) / "all_model_coefficients.csv"
    for t in ["GDP", "NIKKEI", "USD_JPY"]:
        plot_exog_trends(output_path, CONFIG["output_dir"], target_var=t)

if __name__ == "__main__":
    main()